# -*- coding: utf-8 -*-
"""DANN_DCCAN_20+25_epoch_tuned.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Rt77UJ5w-FUytNBRnQb43GGArYMfCx8S

# **Title: Object Detection in Sidescan Sonar Data Using Transfer Learning and Domain Adaptation to Reduce the Need for Manual Annotation**

**Author**
Gomis Kablan Assebian
MSc Data Science Candidate, 2024-2025

---

## **Domain Adaptation: DANN (Domain-Adversarial Neural Network), CDAN (Conditional Domain-Adversarial Network), and Hybrid DCCAN (Domain-Conditional Combined Adversarial Network)**

## **Environment Setup and Repository Access**
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# SETUP: CLONE REPO & INSTALL DOMAIN ADAPTATION LIBRARIES

# Cloning private GitHub
import os
os.environ["GITHUB_TOKEN"] = "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"
!git clone https://${GITHUB_TOKEN}@github.com/Kablan-ASBN/sonar-object-detection.git
# %cd sonar-object-detection

# Install required dependencies
!pip install -r requirements.txt

# Reinstall tllib from official GitHub: needed for DANN & DCCAN losses and trainers
!pip uninstall -y tllib
!pip install git+https://github.com/thuml/Transfer-Learning-Library.git


# PATCH THUML LIBRARY

# Fix broken imports in THUML's ImageList module
!sed -i "s/from torch.utils.data.dataset import Dataset, T_co, IterableDataset/from torch.utils.data import Dataset, IterableDataset/" /usr/local/lib/python3.11/dist-packages/tllib/vision/datasets/imagelist.py
!sed -i "s/class MultipleDomainsDataset(Dataset\\[T_co\\]):/class MultipleDomainsDataset(Dataset):/" /usr/local/lib/python3.11/dist-packages/tllib/vision/datasets/imagelist.py
!sed -i "s/List\\[Dataset\\[T_co\\]\\]/List[Dataset]/g" /usr/local/lib/python3.11/dist-packages/tllib/vision/datasets/imagelist.py
!sed -i "s/T_co/int/g" /usr/local/lib/python3.11/dist-packages/tllib/vision/datasets/imagelist.py

# Fix np.float deprecation
!find /usr/local/lib/python3.11/dist-packages/tllib/ -type f -name "*.py" -exec sed -i 's/np\.float/np.float64/g' {} +

"""## **Dataset Cleaning and 10% Target Subset - VOC-style Splits**

This section prepares robust training splits for the sonar VOC dataset and creates a **10% labeled target subset** for semi-supervised or hybrid domain adaptation. It guarantees that every id listed in a split has **both** its image (`.jpg`) and annotation (`.xml`) present to avoid crashes.
"""

import os, random
from pathlib import Path

# Filter a VOC split so it only lists samples that have both files on memory(drive in my case):
def safe_clean_split(voc_root, input_file, output_file):
    img_dir = Path(voc_root) / "JPEGImages"
    ann_dir = Path(voc_root) / "Annotations"
    input_path = Path(voc_root) / "ImageSets/Main" / input_file
    output_path = Path(voc_root) / "ImageSets/Main" / output_file

    valid_ids = []
    for line in input_path.read_text().splitlines():
        img_id = line.strip()
        jpg = img_dir / f"{img_id}.jpg"
        xml = ann_dir / f"{img_id}.xml"
        if jpg.exists() and xml.exists():
            valid_ids.append(img_id)

    output_path.write_text("\n".join(valid_ids))
    print(f"Cleaned: {output_file} — Valid entries: {len(valid_ids)}")


# Clean the source-domain training split by dropping IDs with missing JPEG/XML
safe_clean_split(
    voc_root="/content/drive/MyDrive/sonar-object-detection/data/line2voc_preprocessed",
    input_file="train.txt",
    output_file="train_source_cleaned.txt"
)

# Clean the target-domain training split
safe_clean_split(
    voc_root="/content/drive/MyDrive/sonar-object-detection/data/line2voc",
    input_file="train.txt",
    output_file="train_target_cleaned.txt"
)

# Build a 10% labeled subset from the cleaned target split.
# Fixed seed for reproducibility; enforce at least one sample.
target_cleaned_path = Path("/content/drive/MyDrive/sonar-object-detection/data/line2voc/ImageSets/Main/train_target_cleaned.txt")
target_subset_path = Path("/content/drive/MyDrive/sonar-object-detection/data/line2voc/ImageSets/Main/train_target_10.txt")

all_target_ids = target_cleaned_path.read_text().splitlines()
subset_size = max(1, int(0.10 * len(all_target_ids)))

random.seed(42)
subset_ids = random.sample(all_target_ids, subset_size)

target_subset_path.write_text("\n".join(subset_ids))
print(f"Saved 10% subset to: {target_subset_path.name} — {len(subset_ids)} entries")

# Re-validate the 10% subset in case files changed: keep IDs with both JPEG and XML.
safe_clean_split(
    voc_root="/content/drive/MyDrive/sonar-object-detection/data/line2voc",
    input_file="train_target_10.txt",
    output_file="train_target_10_cleaned.txt"
)

"""## **DANN Training — Faster R-CNN (3 Classes) with Gradient Reversal**

This section trains a **Domain-Adversarial Neural Network (DANN)** on sonar data using **Faster R-CNN with a ResNet-50 FPN**. The detector learns the task on **labeled source (denoised)** images while the backbone is pushed toward **domain-invariant features** by a domain discriminator trained on **pooled features** from both source and **unlabeled target (raw)** images. A **warm logistic schedule** ramps the domain weight during training so early epochs focus on detection and later epochs emphasize alignment.
"""

# Importing the libraries
import os, math
from pathlib import Path
from contextlib import nullcontext

import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.amp import autocast, GradScaler

import torchvision
import torchvision.transforms.v2 as T2
from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor

from PIL import Image
import xml.etree.ElementTree as ET
from tqdm import tqdm
import pandas as pd
import numpy as np
if not hasattr(np, "float"):
    np.float = np.float64
if not hasattr(np, "int"):
    np.int = np.int64
if not hasattr(np, "bool"):
    np.bool = np.bool_

# THUML components: domain discriminator & warm-start GRL
from tllib.modules.domain_discriminator import DomainDiscriminator
from tllib.modules.grl import WarmStartGradientReverseLayer


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)
torch.backends.cudnn.benchmark = True


# Split hygiene: keep only IDs that have both JPEG and XML
def regenerate_clean_split(voc_root, original_split, new_name):
    voc_root = Path(voc_root)
    img_dir = voc_root / "JPEGImages"
    ann_dir = voc_root / "Annotations"
    in_path = voc_root / "ImageSets/Main" / original_split
    out_path = voc_root / "ImageSets/Main" / new_name

    valid = []
    for line in in_path.read_text().splitlines():
        img_id = line.strip()
        if (img_dir / f"{img_id}.jpg").exists() and (ann_dir / f"{img_id}.xml").exists():
            valid.append(img_id)

    out_path.write_text("\n".join(valid))
    print(f"Cleaned: {new_name} — Valid entries: {len(valid)}")


# Keep source: preprocessed data and target: raw data, splits clean before training
regenerate_clean_split(
    "/content/drive/MyDrive/sonar-object-detection/data/line2voc_preprocessed",
    "train.txt", "train_source_cleaned.txt"
)
regenerate_clean_split(
    "/content/drive/MyDrive/sonar-object-detection/data/line2voc",
    "train.txt", "train_target_cleaned.txt"
)


# Minimal VOC dataset: bg=0, object=1, shadow=2
class CustomVOCDataset(Dataset):
    def __init__(self, root, image_set, transforms=None):
        self.root = Path(root)
        self.img_dir = self.root / "JPEGImages"
        self.ann_dir = self.root / "Annotations"
        split_path = self.root / "ImageSets/Main" / f"{image_set}.txt"
        self.ids = split_path.read_text().splitlines()
        self.class_map = {"object": 1, "shadow": 2}
        self.transforms = transforms

    def __getitem__(self, idx):
        img_id = self.ids[idx]
        img = Image.open(self.img_dir / f"{img_id}.jpg").convert("RGB")
        ann_path = self.ann_dir / f"{img_id}.xml"

        boxes, labels = [], []
        try:
            root = ET.parse(ann_path).getroot()
            for obj in root.findall("object"):
                name = obj.find("name").text.strip()
                label = self.class_map.get(name, None)
                if label is None:
                    continue
                b = obj.find("bndbox")
                xmin = float(b.find("xmin").text); ymin = float(b.find("ymin").text)
                xmax = float(b.find("xmax").text); ymax = float(b.find("ymax").text)
                if xmax > xmin and ymax > ymin:
                    boxes.append([xmin, ymin, xmax, ymax]); labels.append(label)
        except Exception:
            # If an XML is broken, fall back to a tiny dummy box so the loader keeps going.
            boxes = [[0, 0, 1, 1]]; labels = [0]

        target = {
            "boxes": torch.tensor(boxes, dtype=torch.float32),
            "labels": torch.tensor(labels, dtype=torch.int64),
            "image_id": torch.tensor([idx])
        }
        if self.transforms:
            img = self.transforms(img)
        return img, target

    def __len__(self):
        return len(self.ids)


def collate_fn(batch):
    return list(zip(*batch))


# Transforms: keep normalization consistent with torchvision DEFAULT
weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT
transform = T2.Compose([
    T2.ToImage(),
    T2.RandomHorizontalFlip(0.5),
    weights.transforms(),
])


# Data: source = denoised/preprocessed, target = raw
source_dataset = CustomVOCDataset(
    "/content/drive/MyDrive/sonar-object-detection/data/line2voc_preprocessed",
    "train_source_cleaned", transforms=transform
)
target_dataset = CustomVOCDataset(
    "/content/drive/MyDrive/sonar-object-detection/data/line2voc",
    "train_target_cleaned", transforms=transform
)

BATCH_SIZE = 12
source_loader = DataLoader(source_dataset, batch_size=BATCH_SIZE, shuffle=True,
                           num_workers=2, collate_fn=collate_fn, pin_memory=True)
target_loader = DataLoader(target_dataset, batch_size=BATCH_SIZE, shuffle=True,
                           num_workers=2, collate_fn=collate_fn, pin_memory=True)

print(f"Source: {len(source_dataset)} | Target: {len(target_dataset)}")


# Detector + domain discriminator + GRL
model = fasterrcnn_resnet50_fpn(weights=weights)
in_features = model.roi_heads.box_predictor.cls_score.in_features
model.roi_heads.box_predictor = FastRCNNPredictor(in_features, 3)
model.to(device)

disc = DomainDiscriminator(in_feature=256, hidden_size=1024).to(device)


class GRLDANNLoss(torch.nn.Module):
    def __init__(self, domain_discriminator, alpha=1.0):
        super().__init__()
        self.domain_discriminator = domain_discriminator
        self.grl = WarmStartGradientReverseLayer(alpha=alpha)

    def forward(self, f_s, f_t):
        feats = torch.cat([f_s, f_t], dim=0)
        feats = self.grl(feats)

        labels = torch.cat([
            torch.ones(f_s.size(0), 1, device=f_s.device),
            torch.zeros(f_t.size(0), 1, device=f_t.device)
        ], dim=0)

        logits = self.domain_discriminator(feats)
        return F.binary_cross_entropy_with_logits(logits, labels)


dann_loss_fn = GRLDANNLoss(disc, alpha=1.0).to(device)


# Optimizer & AMP
optimizer = torch.optim.SGD([
    {"params": model.parameters(), "lr": 1e-3, "momentum": 0.9, "weight_decay": 5e-4},
    {"params": disc.parameters(),  "lr": 2e-3, "momentum": 0.9, "weight_decay": 5e-4},
])
scaler = GradScaler()


# Briefly freeze BN to stabilize early epochs
FREEZE_BN_EPOCHS = 2
def freeze_backbone_bn(m):
    if isinstance(m, torch.nn.BatchNorm2d):
        m.eval()
        for p in m.parameters():
            p.requires_grad_(False)


# DANN ramp
def dann_lambda(progress: float) -> float:
    return 2.0 / (1.0 + math.exp(-10.0 * progress)) - 1.0


# Trainning phase
EPOCHS = 20
loss_log = []
model.train(); disc.train()

for epoch in range(1, EPOCHS + 1):
    if epoch <= FREEZE_BN_EPOCHS:
        model.backbone.apply(freeze_backbone_bn)

    total_det, total_dom = 0.0, 0.0
    n_steps = min(len(source_loader), len(target_loader))

    loop = tqdm(zip(source_loader, target_loader), total=n_steps,
                desc=f"Epoch {epoch}/{EPOCHS}", ncols=100)

    for step, ((src_imgs, src_tgts), (tgt_imgs, _)) in enumerate(loop, 1):
        # Move tensors to device; pin_memory + non_blocking helps overlap device
        src_imgs = [img.to(device, non_blocking=True) for img in src_imgs]
        tgt_imgs = [img.to(device, non_blocking=True) for img in tgt_imgs]
        src_tgts = [{k: v.to(device, non_blocking=True) for k, v in t.items()} for t in src_tgts]

        # Progress for lambda schedule across the whole run
        p = ((epoch - 1) + (step - 1) / n_steps) / EPOCHS
        lambda_dom = dann_lambda(p)

        # Mixed precision on CUDA; plain context on CPU
        amp_ctx = autocast("cuda") if device.type == "cuda" else nullcontext()
        with amp_ctx:
            # 1) Supervised detection loss on source
            det_loss = sum(model(src_imgs, src_tgts).values())

            # 2) Domain loss on pooled backbone features
            f_s = model.backbone(torch.stack(src_imgs))["0"]
            f_t = model.backbone(torch.stack(tgt_imgs))["0"]
            f_s = F.adaptive_avg_pool2d(f_s, 1).squeeze(-1).squeeze(-1)
            f_t = F.adaptive_avg_pool2d(f_t, 1).squeeze(-1).squeeze(-1)
            dom_loss = dann_loss_fn(f_s, f_t)

            # 3) Final objective
            loss = det_loss + lambda_dom * dom_loss

        optimizer.zero_grad(set_to_none=True)
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        total_det += det_loss.item()
        total_dom += dom_loss.item()

        # I log the raw domain loss (before scaling) along with the current lambda.
        loop.set_postfix({
            "Det": f"{det_loss.item():.3f}",
            "Dom": f"{dom_loss.item():.3f}",
            "lambda_dom": f"{lambda_dom:.2f}"
        })

    print(f"Epoch {epoch}: Det={total_det:.2f}, Dom={total_dom:.2f}")
    loss_log.append((epoch, total_det, total_dom))


# Save model and training curve
ckpt_dir = Path("/content/drive/MyDrive/sonar-object-detection/checkpoints")
out_dir  = Path("/content/drive/MyDrive/sonar-object-detection/outputs")
ckpt_dir.mkdir(exist_ok=True); out_dir.mkdir(exist_ok=True)

torch.save(model.state_dict(), ckpt_dir / "dann_sonar_fasterrcnn_20epoch.pth")
pd.DataFrame(loss_log, columns=["epoch", "detection_loss_sum", "domain_loss_sum"]).to_csv(
    out_dir / "dann_loss_curve.csv", index=False
)

print("Training is done, and the model and logs were saved in drive.")

"""## **Batch Inference — DANN Faster R-CNN (20 Epochs) on Raw Sonar**

This section runs **batch inference** with the **DANN-trained Faster R-CNN** on raw sonar images and exports predictions in two modes:

- **RAW CSV** for metrics (closest to detector outputs; no custom post-processing beyond the detector’s own per-class NMS and a top-K cap)  
- **VIS CSV + annotated images** for clean screenshots (adds a human-friendly de-dup pipeline and a score display threshold)

The split between RAW and VIS lets you keep evaluation standard-compliant while producing readable figures for the thesis and viva.
"""

# Batch inference for DANN Faster R-CNN 20 epochs on raw sonar data

import cv2, torch, pandas as pd, numpy as np
from PIL import Image
from pathlib import Path
from tqdm import tqdm

from torchvision.transforms import ToTensor
from torchvision.ops import nms, box_iou
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.models.detection.faster_rcnn import (
    FastRCNNPredictor, FasterRCNN_ResNet50_FPN_Weights
)

# Paths and files
PROJECT_ROOT = Path("/content/drive/MyDrive/sonar-object-detection")
IMG_DIR      = PROJECT_ROOT / "data" / "line2voc" / "JPEGImages"
MODEL_PATH   = PROJECT_ROOT / "checkpoints" / "dann_sonar_fasterrcnn_20epoch.pth"

# Where I save things
OUTPUT_DIR   = PROJECT_ROOT / "outputs" / "vis_dann_20epoch"  # visuals predictions only
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

RAW_CSV = PROJECT_ROOT / "outputs" / "preds_dann_20epoch_RAW.csv"  # this is used for scoring
VIS_CSV = PROJECT_ROOT / "outputs" / "preds_dann_20epoch_VIS.csv"  # this is used for visuals

# Device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

# Visual-only post-processing
SCORE_THRESH         = 0.50   # final display/export cut for VIS
NMS_IOU_CLASSWISE    = 0.50   # class-wise NMS
MERGE_IOU_CLASSWISE  = 0.60   # same-class box fusion threshold
CROSS_CLASS_IOU      = 0.85   # suppress near-duplicates across classes
DO_GREEDY_FUSION     = True
DO_CROSS_CLASS_SUPPR = True
MAX_DETS_PER_IMAGE   = 300

# Drawing palette
CLASS_NAMES  = {1: "object", 2: "shadow"}
CLASS_COLORS = {1: (0, 255, 0), 2: (0, 0, 255)}

# Model: match training setup
weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT
model = fasterrcnn_resnet50_fpn(weights=weights)
in_features = model.roi_heads.box_predictor.cls_score.in_features
model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes=3)

# Load the DANN detector
state = torch.load(MODEL_PATH, map_location=device)
if isinstance(state, dict) and "model" in state:
    state = state["model"]
model.load_state_dict(state)

# Don’t silently prune inside ROIHeads; I control filtering outside
model.roi_heads.score_thresh = 0.0
model.roi_heads.nms_thresh = 0.5
model.roi_heads.detections_per_img = MAX_DETS_PER_IMAGE

model.to(device).eval()
to_tensor = ToTensor()

# helpers for VIS-only cleanup

def _classwise_nms(boxes, scores, labels, iou_thr):

    if boxes.numel() == 0:
        return boxes, scores, labels
    keep_all = []
    for c in labels.unique().tolist():
        idx = torch.nonzero(labels == c, as_tuple=False).squeeze(1)
        if idx.numel() == 0:
            continue
        kept = nms(boxes[idx], scores[idx], iou_thr)
        keep_all.append(idx[kept])
    if not keep_all:
        return boxes[:0], scores[:0], labels[:0]
    keep = torch.cat(keep_all, dim=0)
    return boxes[keep], scores[keep], labels[keep]

def _greedy_merge_per_class(boxes, scores, labels, iou_thr):
    if boxes.numel() == 0:
        return boxes, scores, labels
    out_b, out_s, out_l = [], [], []
    for c in labels.unique().tolist():
        idx = torch.nonzero(labels == c, as_tuple=False).squeeze(1)
        if idx.numel() == 0:
            continue
        B, S = boxes[idx].clone(), scores[idx].clone()
        order = torch.argsort(S, descending=True)
        used = torch.zeros(order.numel(), dtype=torch.bool)
        for r, j in enumerate(order):
            if used[r]:
                continue
            base = B[j].unsqueeze(0)
            ious = box_iou(base, B[order])[0]
            mask = (ious >= iou_thr) & (~used)
            members = order[mask]
            w = S[members]
            fused = (B[members] * w[:, None]).sum(0) / (w.sum() + 1e-8)
            out_b.append(fused)
            out_s.append(S[members].max())
            out_l.append(torch.tensor(c, dtype=labels.dtype))
            used |= mask
    return torch.stack(out_b), torch.stack(out_s), torch.stack(out_l)

def _cross_class_suppress(boxes, scores, labels, iou_thr):
    if boxes.numel() == 0:
        return boxes, scores, labels
    order = torch.argsort(scores, descending=True)
    keep = torch.ones(order.numel(), dtype=torch.bool)
    for i in range(order.numel()):
        if not keep[i]:
            continue
        bi, li = boxes[order[i]].unsqueeze(0), labels[order[i]]
        rest = order[i+1:]
        if rest.numel() == 0:
            continue
        ious = box_iou(bi, boxes[rest])[0]
        clash = (ious >= iou_thr) & (labels[rest] != li)
        keep[i+1:][clash] = False
    final = order[keep]
    return boxes[final], scores[final], labels[final]

def _enforce_topk(boxes, scores, labels, k):
    if scores.numel() <= k:
        return boxes, scores, labels
    idx = torch.argsort(scores, descending=True)[:k]
    return boxes[idx], scores[idx], labels[idx]

# Export in two flavors:
#   - RAW: dump model outputs. Used for metrics.
#   - VIS: light de-dup + draw boxes. Used for screenshots.
def export_preds(model, image_dir, out_path, mode="RAW"):
    assert mode in {"RAW", "VIS"}
    image_dir = Path(image_dir)

    results = []
    image_paths = sorted(
        list(image_dir.glob("*.jpg")) + list(image_dir.glob("*.JPG")) +
        list(image_dir.glob("*.png")) + list(image_dir.glob("*.PNG"))
    )

    for p in tqdm(image_paths, desc=f"DANN export ({mode})"):
        img = Image.open(p).convert("RGB")
        x = to_tensor(img).unsqueeze(0).to(device)
        with torch.no_grad():
            out = model(x)[0]

        # Work on CPU for the rest of the pipeline
        boxes  = out.get("boxes",  torch.empty((0, 4))).detach().cpu()
        scores = out.get("scores", torch.empty((0,))).detach().cpu()
        labels = out.get("labels", torch.empty((0,), dtype=torch.long)).detach().cpu()

        if mode == "RAW":
            # Metrics path: no score gate, no extra dedup — just respect the model’s cap
            boxes, scores, labels = _enforce_topk(boxes, scores, labels, MAX_DETS_PER_IMAGE)
        else:
            # Pretty path for figures
            boxes, scores, labels = _classwise_nms(boxes, scores, labels, NMS_IOU_CLASSWISE)
            if DO_GREEDY_FUSION and boxes.numel() > 0:
                boxes, scores, labels = _greedy_merge_per_class(boxes, scores, labels, MERGE_IOU_CLASSWISE)
            if DO_CROSS_CLASS_SUPPR and boxes.numel() > 0:
                boxes, scores, labels = _cross_class_suppress(boxes, scores, labels, CROSS_CLASS_IOU)
            boxes, scores, labels = _enforce_topk(boxes, scores, labels, MAX_DETS_PER_IMAGE)
            keep = scores >= SCORE_THRESH
            boxes, scores, labels = boxes[keep], scores[keep], labels[keep]

            # Annotated image for VIS
            im = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)
            for b, s, l in zip(boxes, scores, labels):
                x1, y1, x2, y2 = map(int, b.tolist())
                color = CLASS_COLORS.get(int(l), (255, 255, 255))
                name  = CLASS_NAMES.get(int(l), "unknown")
                cv2.rectangle(im, (x1, y1), (x2, y2), color, 2)
                cv2.putText(im, f"{name} ({float(s):.2f})", (x1, y1 - 5),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
            cv2.imwrite(str(OUTPUT_DIR / p.name), im)

        # Collect rows
        for b, s, l in zip(boxes, scores, labels):
            x1, y1, x2, y2 = map(float, b.tolist())
            results.append({
                "filename": p.name,
                "class_id": int(l),
                "class_name": CLASS_NAMES.get(int(l), "unknown"),
                "score": float(s),
                "xmin": x1, "ymin": y1, "xmax": x2, "ymax": y2,
            })

    pd.DataFrame(results).to_csv(out_path, index=False)
    print(f"{mode} predictions -> {out_path}")
    if mode == "VIS":
        print(f"Annotated images -> {OUTPUT_DIR}")

# Run both paths
if __name__ == "__main__":
    export_preds(model, IMG_DIR, RAW_CSV, mode="RAW")
    export_preds(model, IMG_DIR, VIS_CSV, mode="VIS")

"""## **COCO Evaluation: DANN Faster R-CNN (20 Epochs)**

This cell performs **direct COCO evaluation** of the DANN-trained Faster R-CNN on a chosen VOC split using TorchMetrics.  
It **does not** export CSVs or annotated figures. (RAW/VIS exports and parity checks belong in separate cells.)
"""

# Evaluation for DANN Faster R-CNN 20 epochs

import sys
from pathlib import Path

import torch
from torch.utils.data import DataLoader
from torchvision.transforms import ToTensor
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.models.detection.faster_rcnn import (
    FastRCNNPredictor, FasterRCNN_ResNet50_FPN_Weights
)
from torchmetrics.detection.mean_ap import MeanAveragePrecision

# CONFIG
PROJECT_ROOT = Path("/content/drive/MyDrive/sonar-object-detection")
DATA_ROOT    = PROJECT_ROOT / "data" / "line2voc"
IMAGE_SET    = "val"
CKPT_PATH    = PROJECT_ROOT / "checkpoints" / "dann_sonar_fasterrcnn_20epoch.pth"

CLASS_NAME   = {1: "object", 2: "shadow"}
MAX_DETS_PER_IMAGE = 300

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
torch.backends.cudnn.benchmark = True
print("Device:", device)

# Dataset wrapper
sys.path.append(str(PROJECT_ROOT / "scripts"))
from voc_dataset import VOCDataset

# Build model
def build_model(checkpoint_path: Path):
    weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT
    model = fasterrcnn_resnet50_fpn(weights=weights)
    in_feats = model.roi_heads.box_predictor.cls_score.in_features
    model.roi_heads.box_predictor = FastRCNNPredictor(in_feats, num_classes=3)
    model.roi_heads.score_thresh = 0.0
    model.roi_heads.nms_thresh = 0.5
    model.roi_heads.detections_per_img = MAX_DETS_PER_IMAGE

    state = torch.load(str(checkpoint_path), map_location=device)
    if isinstance(state, dict) and "model" in state:
        state = state["model"]
    model.load_state_dict(state)
    return model.to(device).eval()

# Evaluation
def coco_eval_from_model(model, dataset_root, image_set, batch_size=4, num_workers=2):
    ds = VOCDataset(str(dataset_root), image_set=image_set, transforms=ToTensor())
    dl = DataLoader(ds, batch_size=batch_size, shuffle=False,
                    collate_fn=lambda x: tuple(zip(*x)), num_workers=num_workers, pin_memory=True)
    metric = MeanAveragePrecision(class_metrics=True)

    for imgs, targets in dl:
        imgs = [img.to(device) for img in imgs]
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]
        with torch.no_grad():
            outs = model(imgs)
        preds, gts = [], []
        for o, t in zip(outs, targets):
            preds.append({"boxes": o["boxes"].cpu(),
                          "scores": o["scores"].cpu(),
                          "labels": o["labels"].cpu()})
            gts.append({"boxes": t["boxes"].cpu(), "labels": t["labels"].cpu()})
        metric.update(preds, gts)

    res = metric.compute()
    print("\n-- Direct COCO (raw model outputs) --")
    for k, v in res.items():
        if isinstance(v, torch.Tensor) and v.ndim == 0:
            print(f"{k}: {float(v):.4f}")
    return res

# Main
if __name__ == "__main__":
    model = build_model(CKPT_PATH)
    direct_res = coco_eval_from_model(model, DATA_ROOT, IMAGE_SET, batch_size=8, num_workers=2)

"""## **Visualize Predictions: DANN 20 Epochs on Raw Target with Validation Split**

This cell renders **readable example images** from the raw target **validation** split using **DANN-trained Faster R-CNN (ResNet-50 FPN)**. It rebuilds the detector exactly as in training, loads the DANN checkpoint, and overlays predicted boxes and scores. The figures are **for qualitative inspection only** (not used for metrics).
"""

# VISUALIZE PREDICTIONS: DANN 20 epochs, raw target split

import sys
import torch
import cv2
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from torchvision.transforms import ToTensor
from torchvision.ops import nms
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.models.detection.faster_rcnn import (
    FastRCNNPredictor,
    FasterRCNN_ResNet50_FPN_Weights,
)


# Config
CONF_THRESH   = 0.50    # draw boxes with score >= this
IOU_THRESH    = 0.65    # IoU used by the optional extra NMS
NMS_MODE      = "classwise"
NUM_IMAGES    = 5

# Paths
PROJECT_ROOT  = Path("/content/drive/MyDrive/sonar-object-detection")
DATASET_ROOT  = str(PROJECT_ROOT / "data" / "line2voc")
CKPT          = str(PROJECT_ROOT / "checkpoints" / "dann_sonar_fasterrcnn_20epoch.pth")

# Labels and colors
LABEL_NAMES   = {1: "object", 2: "shadow"}
LABEL_COLORS  = {1: (0, 255, 0), 2: (0, 0, 255)}

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# Dataset wrapper
sys.path.append(str(PROJECT_ROOT / "scripts"))
from voc_dataset import VOCDataset


# Helper: optional extra NMS for visuals only
def apply_extra_nms(boxes, scores, labels, iou_thresh=0.5, mode="agnostic"):
    if boxes.numel() == 0:
        return torch.empty((0,), dtype=torch.long)

    if mode == "classwise":
        keep_idxs = []
        for cls in torch.unique(labels):
            mask = labels == cls
            if mask.any():
                kept = nms(boxes[mask], scores[mask], iou_thresh)
                keep_idxs.append(torch.where(mask)[0][kept])
        return torch.cat(keep_idxs) if keep_idxs else torch.empty((0,), dtype=torch.long)

    # default: class-agnostic
    return nms(boxes, scores, iou_thresh)


# Model
weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT
model = fasterrcnn_resnet50_fpn(weights=weights)

# Swap head to 3 classes: bg(0), object(1), shadow(2)
in_features = model.roi_heads.box_predictor.cls_score.in_features
model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes=3)

# Load trained DANN weights and move to eval
state = torch.load(CKPT, map_location=device)
# tolerate either a bare state_dict or {"model": state_dict}
if isinstance(state, dict) and "model" in state:
    state = state["model"]
model.load_state_dict(state)

# Hygiene for VIS ONLY
model.roi_heads.score_thresh = 0.0
model.roi_heads.nms_thresh = 0.5
model.roi_heads.detections_per_img = 300

model.to(device).eval()


# Visualize a few val images
def visualize(model, dataset_root, num_images, conf_thresh, iou_thresh, nms_mode):
    ds = VOCDataset(dataset_root, image_set="val", transforms=ToTensor())

    for idx in range(min(num_images, len(ds))):
        img, _ = ds[idx]
        x = img.unsqueeze(0).to(device)

        with torch.no_grad():
            out = model(x)[0]

        # Optional de-dup for visuals
        keep = apply_extra_nms(out["boxes"], out["scores"], out["labels"],
                               iou_thresh=iou_thresh, mode=nms_mode)
        boxes  = out["boxes"][keep]
        scores = out["scores"][keep]
        labels = out["labels"][keep]

        # Convert back to OpenCV BGR for drawing
        im = cv2.cvtColor(
            (img.mul(255).byte().permute(1, 2, 0).cpu().numpy()),
            cv2.COLOR_RGB2BGR
        )

        shown = 0
        for b, l, s in zip(boxes, labels, scores):
            s = float(s)
            if s < conf_thresh:
                continue
            x1, y1, x2, y2 = b.int().tolist()
            color = LABEL_COLORS.get(int(l), (255, 255, 255))
            name  = LABEL_NAMES.get(int(l), "unknown")

            cv2.rectangle(im, (x1, y1), (x2, y2), color, 2)
            cv2.putText(im, f"{name} {s:.2f}", (x1, max(10, y1 - 6)),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
            shown += 1

        print(
            f"Image {idx+1}: drew {shown} boxes (score ≥ {conf_thresh:.2f}) "
            f"after {nms_mode} NMS at IoU={iou_thresh:.2f}"
        )

        # Show figure
        plt.figure(figsize=(10, 6))
        plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))
        plt.axis("off")
        plt.title(f"DANN (20e) Image {idx+1} | {nms_mode} NMS IoU={iou_thresh}")
        plt.tight_layout(pad=0)
        plt.show()


# Run
visualize(model, DATASET_ROOT, NUM_IMAGES, CONF_THRESH, IOU_THRESH, NMS_MODE)

"""## **DCCAN Model Training: Hybrid Domain Adaptation (DANN + CDAN + RPN-level) for Faster R-CNN (3 Classes)**

This training script implements **DCCAN**, a hybrid domain adaptation setup on top of **Faster R-CNN (ResNet-50 FPN)** with a 3-class head (`bg=0, object=1, shadow=2`).  
It aligns **denoised, labeled source** to **raw, unlabeled target** using three adversarial signals while preserving the detector’s native training pipeline and using **mixed precision**.
"""

# DCCAN training 20e
import sys
from pathlib import Path

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.amp import autocast, GradScaler
from torch.utils.data import DataLoader

import torchvision.transforms.v2 as T2
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.models.detection.faster_rcnn import (
    FastRCNNPredictor,
    FasterRCNN_ResNet50_FPN_Weights,
)

# THUML discriminator blocks
from tllib.modules.domain_discriminator import DomainDiscriminator

import pandas as pd
from tqdm import tqdm
import math
import random

# Basic config
device        = torch.device("cuda" if torch.cuda.is_available() else "cpu")
BATCH_SIZE    = 12
EPOCHS        = 20
FREEZE_EPOCHS = 2              # freeze backbone first 2 epochs with detector head only, no adaptation pressure
NUM_CLASSES   = 3              # bg=0, object=1, shadow=2
F_DIM         = 256
SEED          = 1337

# Adaptation strength
DANN_LAMBDA_MAX = 0.20
CDAN_LAMBDA_MAX = 0.30
RPN_LAMBDA_MAX  = 0.10

# Temperature for CDAN conditioning and confidence gating schedule
CDAN_TEMP        = 0.6         # logits temperature
CONF_THR_START   = 0.40        # only use samples with max prob >= thr early
CONF_THR_END     = 0.20        # relax over time

# Stability
MAX_NORM_GRAD    = 5.0         # gradient clipping to help precision by avoiding noisy updates

torch.backends.cudnn.benchmark = True

# Reproducibility
def set_seed(s=SEED):
    random.seed(s)
    torch.manual_seed(s)
    torch.cuda.manual_seed_all(s)
    torch.use_deterministic_algorithms(False)

set_seed()

# Dataset & collate
sys.path.append("/content/drive/MyDrive/sonar-object-detection/scripts")
from voc_dataset import VOCDataset as CustomVOCDataset

def collate_fn(batch):
    return list(zip(*batch))

# keep transforms minimal
transform = T2.Compose([
    T2.ToImage(),
    T2.ToDtype(torch.float32, scale=True),
])

source_dataset = CustomVOCDataset(
    "/content/drive/MyDrive/sonar-object-detection/data/line2voc_preprocessed",
    "train_source_cleaned",
    transforms=transform,
)
target_dataset = CustomVOCDataset(
    "/content/drive/MyDrive/sonar-object-detection/data/line2voc",
    "train_target_cleaned",
    transforms=transform,
)

source_loader = DataLoader(source_dataset, BATCH_SIZE, True, num_workers=2,
                           collate_fn=collate_fn, pin_memory=True, drop_last=True)
target_loader = DataLoader(target_dataset, BATCH_SIZE, True, num_workers=2,
                           collate_fn=collate_fn, pin_memory=True, drop_last=True)


# Detector: Faster R-CNN (3 classes)
weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT
model = fasterrcnn_resnet50_fpn(weights=weights)
in_features = model.roi_heads.box_predictor.cls_score.in_features
model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES)
model.to(device)

# Proxy head to get logits from pooled features for CDAN
proxy_cls = nn.Linear(F_DIM, NUM_CLASSES).to(device)

# Domain discriminators
# DANN head: global pooled features
disc_dann = DomainDiscriminator(F_DIM, 1024).to(device)
# CDAN head: flattened outer-product
disc_cdan = DomainDiscriminator(F_DIM * NUM_CLASSES, 1024).to(device)

# RPN-level alignment: tiny conv head operating on FPN P3 feature maps
class RPNDomainHead(nn.Module):
    def __init__(self, in_ch=F_DIM):
        super().__init__()
        self.conv1 = nn.Conv2d(in_ch, 64, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(64, 1, kernel_size=1)
        self.act   = nn.ReLU(inplace=True)
    def forward(self, x):
        h = self.act(self.conv1(x))
        return self.conv2(h)

rpn_domain_head = RPNDomainHead().to(device)

# Gradient Reversal Layer with dynamic coeff
class _GradReverse(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, coeff):
        ctx.coeff = coeff
        return x.view_as(x)
    @staticmethod
    def backward(ctx, g):
        return -ctx.coeff * g, None

class GRL(nn.Module):
    def __init__(self, coeff=0.0):
        super().__init__()
        self.coeff = float(coeff)
    def set_coeff(self, c: float):
        self.coeff = float(c)
    def forward(self, x):
        return _GradReverse.apply(x, self.coeff)

grl_global = GRL(0.0).to(device)  # for DANN
grl_cdan   = GRL(0.0).to(device)  # for CDAN mapping
grl_rpn    = GRL(0.0).to(device)  # for RPN-level alignment

# Losses
bce_logits = nn.BCEWithLogitsLoss(reduction="mean")

def dann_loss(feat_s, feat_t):
    # feat_*: [B, 256]
    feats = torch.cat([feat_s, feat_t], dim=0)
    feats = grl_global(feats)
    logits = disc_dann(feats)
    labels = torch.cat([
        torch.ones((feat_s.size(0), 1), device=feats.device),
        torch.zeros((feat_t.size(0), 1), device=feats.device)
    ], dim=0)
    loss = bce_logits(logits, labels)
    with torch.no_grad():
        pred = (torch.sigmoid(logits) >= 0.5).float()
        acc = (pred.eq(labels)).float().mean().item()
    return loss, acc

def l2_normalize_rows(x, eps=1e-6):
    # x: [B, D]
    return x / (x.norm(p=2, dim=1, keepdim=True) + eps)

def cdan_loss(g_s, f_s, g_t, f_t, temp=CDAN_TEMP, conf_thr=0.3):

    # softmax with temperature, detach to avoid pushing classifier by domain loss
    p_s = F.softmax(g_s / temp, dim=1).detach()
    p_t = F.softmax(g_t / temp, dim=1).detach()

    # L2 normalize
    f_s_n = l2_normalize_rows(f_s)
    f_t_n = l2_normalize_rows(f_t)
    p_s_n = l2_normalize_rows(p_s)
    p_t_n = l2_normalize_rows(p_t)

    # outer product per sample, then flatten to [B, 256*C]
    def outer_flat(f, p):
        B, D = f.shape
        h = torch.bmm(f.unsqueeze(2), p.unsqueeze(1))
        return h.reshape(B, D * p.shape[1])

    h_s = outer_flat(f_s_n, p_s_n)
    h_t = outer_flat(f_t_n, p_t_n)
    h   = torch.cat([h_s, h_t], dim=0)
    h   = grl_cdan(h)

    logits = disc_cdan(h)
    labels = torch.cat([
        torch.ones((g_s.size(0), 1), device=logits.device),
        torch.zeros((g_t.size(0), 1), device=logits.device)
    ], dim=0)

    # confidence gating
    conf = torch.cat([p_s.max(dim=1).values, p_t.max(dim=1).values], dim=0)
    weights = (conf >= conf_thr).float().unsqueeze(1)
    denom = weights.sum().clamp(min=1.0)
    loss = F.binary_cross_entropy_with_logits(logits, labels, weight=weights, reduction='sum') / denom

    with torch.no_grad():
        pred = (torch.sigmoid(logits) >= 0.5).float()
        acc = (pred.eq(labels)).float().mean().item()
        kept = int(denom.item())
    return loss, acc, kept

def rpn_domain_loss(p3_s, p3_t):

    # p3_*: [B, 256, H, W]
    x = torch.cat([p3_s, p3_t], dim=0)
    x = grl_rpn(x)
    logits_map = rpn_domain_head(x)
    # global average pool to per-sample logits
    logits = logits_map.mean(dim=[2, 3])
    labels = torch.cat([
        torch.ones((p3_s.size(0), 1), device=logits.device),
        torch.zeros((p3_t.size(0), 1), device=logits.device)
    ], dim=0)
    loss = bce_logits(logits, labels)
    with torch.no_grad():
        pred = (torch.sigmoid(logits) >= 0.5).float()
        acc = (pred.eq(labels)).float().mean().item()
    return loss, acc


# Optimizer with parameter groups
# Detector & proxy head
detector_params = list(model.parameters()) + list(proxy_cls.parameters())
# Domain heads
disc_params = list(disc_dann.parameters()) + list(disc_cdan.parameters()) + list(rpn_domain_head.parameters())

optimizer = torch.optim.SGD([
    {"params": detector_params, "lr": 1.5e-3, "momentum": 0.9, "weight_decay": 5e-4},
    {"params": disc_params,     "lr": 4.5e-4, "momentum": 0.9, "weight_decay": 5e-4},
], lr=1.5e-3, momentum=0.9, weight_decay=5e-4)

scaler = GradScaler()


# Utilities: schedule and freezing
def grl_schedule(progress):

    base = 2.0 / (1.0 + math.exp(-10.0 * progress)) - 1.0
    return base

def conf_thr_schedule(progress):
    if progress <= 0.5:
        alpha = progress / 0.5
        return CONF_THR_START + (CONF_THR_END - CONF_THR_START) * alpha
    else:
        return CONF_THR_END

def set_backbone_trainable(module, flag: bool):
    for p in module.parameters():
        p.requires_grad = flag


# Training
model.train(); proxy_cls.train(); disc_dann.train(); disc_cdan.train(); rpn_domain_head.train()

total_steps = min(len(source_loader), len(target_loader)) * EPOCHS
global_step = 0

log_rows = []

for epoch in range(1, EPOCHS + 1):
    # backbone warmup: first FREEZE_EPOCHS epochs -> train only detector heads without adaptation
    backbone_trainable = (epoch > FREEZE_EPOCHS)
    set_backbone_trainable(model.backbone, backbone_trainable)

    sum_det = sum_dann = sum_cdan = sum_rpn = 0.0
    sum_dacc = sum_cacc = sum_racc = 0.0
    sum_cdan_kept = 0
    nb = 0

    loop = tqdm(
        zip(source_loader, target_loader),
        total=min(len(source_loader), len(target_loader)),
        desc=f"Epoch {epoch}/{EPOCHS}"
    )

    for (src_imgs, src_tgts), (tgt_imgs, _) in loop:
        nb += 1
        global_step += 1
        progress = global_step / total_steps

        # GRL coefficients
        ramp = grl_schedule(progress)
        grl_global.set_coeff( (DANN_LAMBDA_MAX if backbone_trainable else 0.0) * ramp )
        grl_cdan.set_coeff(   (CDAN_LAMBDA_MAX if backbone_trainable else 0.0) * ramp )
        grl_rpn.set_coeff(    (RPN_LAMBDA_MAX  if backbone_trainable else 0.0) * ramp )
        conf_thr = conf_thr_schedule(progress)

        # to device
        src_imgs = [img.to(device, non_blocking=True) for img in src_imgs]
        tgt_imgs = [img.to(device, non_blocking=True) for img in tgt_imgs]
        src_tgts = [{k: v.to(device, non_blocking=True) for k, v in t.items()} for t in src_tgts]

        with autocast(device_type="cuda", enabled=(device.type == "cuda")):
            # 1) supervised detection loss on source
            det_loss = sum(model(src_imgs, src_tgts).values())

            # 2) Extract features using the SAME preprocessing as the detector path
            s_img_list, _ = model.transform(src_imgs, None)
            t_img_list, _ = model.transform(tgt_imgs, None)

            p3_s = model.backbone(s_img_list.tensors)["0"]
            p3_t = model.backbone(t_img_list.tensors)["0"]

            f_s_pool = F.adaptive_avg_pool2d(p3_s, 1).reshape(p3_s.size(0), -1)
            f_t_pool = F.adaptive_avg_pool2d(p3_t, 1).reshape(p3_t.size(0), -1)

            # 3) Proxy logits for CDAN conditioning
            g_s = proxy_cls(f_s_pool)
            g_t = proxy_cls(f_t_pool)

            # 4) Domain losses
            if backbone_trainable:
                dann_l, dann_acc = dann_loss(f_s_pool, f_t_pool)
                cdan_l, cdan_acc, cdan_kept = cdan_loss(g_s, f_s_pool, g_t, f_t_pool, temp=CDAN_TEMP, conf_thr=conf_thr)
                rpn_l,  rpn_acc  = rpn_domain_loss(p3_s, p3_t)
            else:
                # no adaptation pressure during warmup
                dann_l = cdan_l = rpn_l = torch.tensor(0.0, device=device)
                dann_acc = cdan_acc = rpn_acc = 0.0
                cdan_kept = 0

            # 5) Total loss
            loss = det_loss + dann_l + cdan_l + rpn_l

        optimizer.zero_grad(set_to_none=True)
        scaler.scale(loss).backward()
        # stability: clip grads to curb noisy updates and improve precision
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(detector_params, MAX_NORM_GRAD)
        torch.nn.utils.clip_grad_norm_(disc_params,     MAX_NORM_GRAD)
        scaler.step(optimizer)
        scaler.update()

        # Accumulate logs
        sum_det  += det_loss.item()
        sum_dann += float(dann_l.item())
        sum_cdan += float(cdan_l.item())
        sum_rpn  += float(rpn_l.item())
        sum_dacc += dann_acc
        sum_cacc += cdan_acc
        sum_racc += rpn_acc
        sum_cdan_kept += cdan_kept

        loop.set_postfix({
            "Det": f"{det_loss.item():.3f}",
            "DANN": f"{dann_l.item():.3f}",
            "CDAN": f"{cdan_l.item():.3f}",
            "RPN": f"{rpn_l.item():.3f}",
            "accD": f"{dann_acc:.2f}",
            "accC": f"{cdan_acc:.2f}",
            "accR": f"{rpn_acc:.2f}",
            "conf_thr": f"{conf_thr:.2f}",
            "grlD": f"{grl_global.coeff:.2f}",
            "grlC": f"{grl_cdan.coeff:.2f}",
            "grlR": f"{grl_rpn.coeff:.2f}",
        })

    # epoch summary
    n = max(nb, 1)
    print(f"Epoch {epoch}: Det {sum_det:.2f}  DANN {sum_dann:.2f} (acc {sum_dacc/n:.2f})  "
          f"CDAN {sum_cdan:.2f} (acc {sum_cacc/n:.2f}, kept {int(sum_cdan_kept/n)})  "
          f"RPN {sum_rpn:.2f} (acc {sum_racc/n:.2f})")

    log_rows.append({
        "epoch": epoch,
        "det_loss": sum_det,
        "dann_loss": sum_dann,
        "cdan_loss": sum_cdan,
        "rpn_loss": sum_rpn,
        "dann_acc": sum_dacc / n,
        "cdan_acc": sum_cacc / n,
        "rpn_acc": sum_racc / n,
    })


# Save artifacts
ckpt_dir = Path("/content/drive/MyDrive/sonar-object-detection/checkpoints")
out_dir  = Path("/content/drive/MyDrive/sonar-object-detection/outputs")
ckpt_dir.mkdir(exist_ok=True, parents=True)
out_dir.mkdir(exist_ok=True, parents=True)

torch.save({
    "model":     model.state_dict(),
    "proxy_cls": proxy_cls.state_dict(),
    "disc_dann": disc_dann.state_dict(),
    "disc_cdan": disc_cdan.state_dict(),
    "rpn_head":  rpn_domain_head.state_dict(),
    "cfg": {
        "EPOCHS": EPOCHS,
        "FREEZE_EPOCHS": FREEZE_EPOCHS,
        "DANN_LAMBDA_MAX": DANN_LAMBDA_MAX,
        "CDAN_LAMBDA_MAX": CDAN_LAMBDA_MAX,
        "RPN_LAMBDA_MAX": RPN_LAMBDA_MAX,
        "CDAN_TEMP": CDAN_TEMP,
    }
}, ckpt_dir / "dccan_sonar_fasterrcnn_stabilized_20e.pth")

pd.DataFrame(log_rows).to_csv(out_dir / "dccan_stabilized_loss_curve.csv", index=False)
print("Saved stabilized DCCAN checkpoint and training log.")

"""## **DCCAN Batch Inference: Two Exports (RAW for metrics, VIS for visuals)**

This cell runs **batch inference** with the **DCCAN-trained Faster R-CNN (ResNet-50 FPN, 3 classes)** on the raw target images and exports **two prediction CSVs**:

- **RAW CSV** (`outputs/preds_dccan_20epoch_RAW.csv`): closest to the detector’s native outputs (only a top-K cap). Use this for **COCO** and **FROC** evaluation.
- **VIS CSV + annotated images** (`outputs/preds_dccan_20epoch_VIS.csv`, `outputs/vis_dccan_20epoch/`): adds light, human-oriented de-dup steps for **clean screenshots** only.

The detector is rebuilt exactly as in training, DCCAN **detector weights** are loaded, and ROIHeads thresholds are made explicit so exports are reproducible.
"""

# BATCH INFERENCE: DCCAN Faster R-CNN 20 epochs on RAW sonar
# Exports TWO CSVs:
#   1) preds_dccan_20epoch_RAW.csv  -> should be used in COCO/FROC evaluator
#   2) preds_dccan_20epoch_VIS.csv  -> should be used for visuals
import os, cv2, torch, pandas as pd, numpy as np
from PIL import Image
from pathlib import Path
from tqdm import tqdm

from torchvision.transforms import ToTensor
from torchvision.ops import nms, box_iou
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.models.detection.faster_rcnn import (
    FastRCNNPredictor, FasterRCNN_ResNet50_FPN_Weights
)

# Paths & device
PROJECT_ROOT = Path("/content/drive/MyDrive/sonar-object-detection")
IMG_DIR      = PROJECT_ROOT / "data" / "line2voc" / "JPEGImages"
CKPT_PATH    = PROJECT_ROOT / "checkpoints" / "dccan_sonar_fasterrcnn_stabilized_20e.pth"

VIS_DIR     = PROJECT_ROOT / "outputs" / "vis_dccan_20epoch"
OUT_DIR     = PROJECT_ROOT / "outputs"
RAW_CSV     = OUT_DIR / "preds_dccan_20epoch_RAW.csv"
VIS_CSV     = OUT_DIR / "preds_dccan_20epoch_VIS.csv"

VIS_DIR.mkdir(parents=True, exist_ok=True)
OUT_DIR.mkdir(parents=True, exist_ok=True)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

# knobs
RAW_MAX_DETS          = 300
RAW_NMS_THR           = 0.50
# VIS pipeline
SCORE_THRESH          = 0.50
NMS_IOU_CLASSWISE     = 0.50
MERGE_IOU_CLASSWISE   = 0.60
CROSS_CLASS_IOU       = 0.95
DO_GREEDY_FUSION      = True
DO_CROSS_CLASS_SUPPR  = True
VIS_MAX_DETS          = 100
# Small-box filter
MIN_W, MIN_H, MIN_AREA = 8, 8, 64
# Optional TTA
DO_TTA_FLIP = False

CLASS_NAMES  = {1: "object", 2: "shadow"}
CLASS_COLORS = {1: (0, 255, 0), 2: (0, 0, 255)}
CLASS_THRESH = {1: 0.60, 2: 0.70}

# Model
weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT
model = fasterrcnn_resnet50_fpn(weights=weights)
in_features = model.roi_heads.box_predictor.cls_score.in_features
model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes=3)

# Load checkpoint + mirror any training resize cfg
state = torch.load(CKPT_PATH, map_location=device)
cfg = {}
if isinstance(state, dict) and "model" in state:
    cfg = state.get("cfg", {})
    state = state["model"]
model.load_state_dict(state)

# Note to who will be using my code after submission:
# If you trained with smaller resize, mirror it here
if cfg.get("USE_SMALLER_RESIZE", False):
    ms = tuple(cfg.get("MIN_SIZE_TRAIN", (800,)))
    mx = int(cfg.get("MAX_SIZE_TRAIN", 1333))
    model.transform.min_size = ms
    model.transform.max_size = mx
    print(f"[info] mirrored train resize -> min={ms}, max={mx}")

model.to(device).eval()
to_tensor = ToTensor()

# helpers
def _clip_and_validate(boxes, W, H):
    if boxes.numel() == 0:
        return boxes, torch.zeros((0,), dtype=torch.bool)
    boxes = boxes.clone()
    boxes[:, [0, 2]] = boxes[:, [0, 2]].clamp(0, W - 1)
    boxes[:, [1, 3]] = boxes[:, [1, 3]].clamp(0, H - 1)
    keep = (boxes[:, 2] > boxes[:, 0]) & (boxes[:, 3] > boxes[:, 1])
    return boxes[keep], keep

def _filter_small(boxes, scores, labels):
    if boxes.numel() == 0:
        return boxes, scores, labels
    wh = boxes[:, 2:4] - boxes[:, 0:2]
    keep = (wh[:, 0] >= MIN_W) & (wh[:, 1] >= MIN_H) & (wh.prod(dim=1) >= MIN_AREA)
    return boxes[keep], scores[keep], labels[keep]

def _classwise_nms(boxes, scores, labels, iou_thr):
    if boxes.numel() == 0:
        return boxes, scores, labels
    keep_all = []
    for c in labels.unique().tolist():
        idx = torch.nonzero(labels == c, as_tuple=False).squeeze(1)
        if idx.numel() == 0:
            continue
        kept = nms(boxes[idx], scores[idx], iou_thr)
        keep_all.append(idx[kept])
    if not keep_all:
        return boxes[:0], scores[:0], labels[:0]
    keep = torch.cat(keep_all, dim=0)
    return boxes[keep], scores[keep], labels[keep]

def _greedy_merge_per_class(boxes, scores, labels, iou_thr):
    if boxes.numel() == 0:
        return boxes, scores, labels
    out_b, out_s, out_l = [], [], []
    for c in labels.unique().tolist():
        idx = torch.nonzero(labels == c, as_tuple=False).squeeze(1)
        if idx.numel() == 0:
            continue
        B, S = boxes[idx].clone(), scores[idx].clone()
        order = torch.argsort(S, descending=True)
        used = torch.zeros(order.numel(), dtype=torch.bool)
        for r, j in enumerate(order):
            if used[r]:
                continue
            base = B[j].unsqueeze(0)
            ious = box_iou(base, B[order])[0]
            mask = (ious >= iou_thr) & (~used)
            members = order[mask]
            w = S[members]
            fused = (B[members] * w[:, None]).sum(0) / (w.sum() + 1e-8)
            out_b.append(fused); out_s.append(S[members].max()); out_l.append(torch.tensor(c, dtype=labels.dtype))
            used |= mask
    return torch.stack(out_b), torch.stack(out_s), torch.stack(out_l)

def _cross_class_suppress(boxes, scores, labels, iou_thr):
    if boxes.numel() == 0:
        return boxes, scores, labels
    order = torch.argsort(scores, descending=True)
    keep = torch.ones(order.numel(), dtype=torch.bool)
    for i in range(order.numel()):
        if not keep[i]:
            continue
        bi, li = boxes[order[i]].unsqueeze(0), labels[order[i]]
        rest = order[i+1:]
        if rest.numel() == 0:
            continue
        ious = box_iou(bi, boxes[rest])[0]
        clash = (ious >= iou_thr) & (labels[rest] != li)
        keep[i+1:][clash] = False
    final = order[keep]
    return boxes[final], scores[final], labels[final]

def _enforce_topk(boxes, scores, labels, k):
    if scores.numel() <= k:
        return boxes, scores, labels
    idx = torch.argsort(scores, descending=True)[:k]
    return boxes[idx], scores[idx], labels[idx]

def _per_class_threshold(boxes, scores, labels, thresh_map, default_thr=0.5):
    if scores.numel() == 0:
        return boxes, scores, labels
    keep = torch.tensor(
        [s >= thresh_map.get(int(l), default_thr) for s, l in zip(scores, labels)],
        dtype=torch.bool
    )
    return boxes[keep], scores[keep], labels[keep]

def _run_model(img_pil):
    x = to_tensor(img_pil).to(device)
    with torch.no_grad():
        out_main = model([x])[0]

    boxes  = out_main.get("boxes",  torch.empty((0, 4), device=device))
    scores = out_main.get("scores", torch.empty((0,),   device=device))
    labels = out_main.get("labels", torch.empty((0,),   device=device, dtype=torch.long))

    if DO_TTA_FLIP:
        W, H = img_pil.size
        x_flip = torch.flip(x, dims=[2])
        with torch.no_grad():
            out_flip = model([x_flip])[0]
        b2  = out_flip.get("boxes",  torch.empty((0, 4), device=device)).clone()
        s2  = out_flip.get("scores", torch.empty((0,),   device=device))
        l2  = out_flip.get("labels", torch.empty((0,),   device=device, dtype=torch.long))
        if b2.numel() > 0:
            x1 = b2[:, 0].clone(); y1 = b2[:, 1].clone()
            x2 = b2[:, 2].clone(); y2 = b2[:, 3].clone()
            b2[:, 0] = (W - x2); b2[:, 2] = (W - x1)
            b2[:, 1] = y1;       b2[:, 3] = y2
        boxes  = torch.cat([boxes, b2], dim=0)
        scores = torch.cat([scores, s2],  dim=0)
        labels = torch.cat([labels, l2],  dim=0)

    return boxes.detach().cpu(), scores.detach().cpu(), labels.detach().cpu()

# Dual-mode export
def export_preds(model, image_dir, out_path, mode="RAW"):
    assert mode in {"RAW", "VIS"}
    image_dir = Path(image_dir)
    paths = sorted(
        list(image_dir.glob("*.jpg")) + list(image_dir.glob("*.JPG")) +
        list(image_dir.glob("*.png")) + list(image_dir.glob("*.PNG"))
    )

    # set per-mode detector caps for consistency
    if mode == "RAW":
        model.roi_heads.score_thresh = 0.0
        model.roi_heads.nms_thresh   = RAW_NMS_THR
        model.roi_heads.detections_per_img = RAW_MAX_DETS
    else:
        model.roi_heads.score_thresh = 0.0
        model.roi_heads.nms_thresh   = RAW_NMS_THR
        model.roi_heads.detections_per_img = VIS_MAX_DETS

    rows = []
    for p in tqdm(paths, desc=f"DCCAN export ({mode})"):
        img = Image.open(p).convert("RGB")
        W, H = img.size

        boxes, scores, labels = _run_model(img)

        # Always clip to image and drop invalids
        boxes, keep_mask = _clip_and_validate(boxes, W, H)
        scores = scores[keep_mask]; labels = labels[keep_mask]

        if mode == "RAW":
            # RAW: NO small-box filter, NO extra NMS and just keep topK
            boxes, scores, labels = _enforce_topk(boxes, scores, labels, RAW_MAX_DETS)
        else:
            # VIS-only refinements
            boxes, scores, labels = _filter_small(boxes, scores, labels)
            boxes, scores, labels = _classwise_nms(boxes, scores, labels, NMS_IOU_CLASSWISE)
            if DO_GREEDY_FUSION and boxes.numel() > 0:
                boxes, scores, labels = _greedy_merge_per_class(boxes, scores, labels, MERGE_IOU_CLASSWISE)
            if DO_CROSS_CLASS_SUPPR and boxes.numel() > 0:
                boxes, scores, labels = _cross_class_suppress(boxes, scores, labels, CROSS_CLASS_IOU)
            boxes, scores, labels = _enforce_topk(boxes, scores, labels, VIS_MAX_DETS)
            boxes, scores, labels = _per_class_threshold(boxes, scores, labels, CLASS_THRESH, default_thr=SCORE_THRESH)

            # draw
            im = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)
            for b, s, l in zip(boxes, scores, labels):
                x1, y1, x2, y2 = map(int, b.tolist())
                color = CLASS_COLORS.get(int(l), (255, 255, 255))
                name  = CLASS_NAMES.get(int(l), "unknown")
                cv2.rectangle(im, (x1, y1), (x2, y2), color, 2)
                cv2.putText(im, f"{name} ({float(s):.2f})", (x1, y1 - 5),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
            cv2.imwrite(str(VIS_DIR / p.name), im)

        # rows
        for b, s, l in zip(boxes, scores, labels):
            x1, y1, x2, y2 = map(float, b.tolist())
            rows.append({
                "filename": p.name,
                "class_id": int(l),
                "class_name": CLASS_NAMES.get(int(l), "unknown"),
                "score": float(s),
                "xmin": x1, "ymin": y1, "xmax": x2, "ymax": y2,
            })

    pd.DataFrame(rows).to_csv(out_path, index=False)
    print(f"{mode} predictions -> {out_path}")
    if mode == "VIS":
        print(f"Annotated images -> {VIS_DIR}")

# Run both exports
if __name__ == "__main__":
    export_preds(model, IMG_DIR, RAW_CSV, mode="RAW")
    export_preds(model, IMG_DIR, VIS_CSV, mode="VIS")

"""## **Evaluation — DCCAN Faster R-CNN (20 Epochs) on `line2voc`**

This cell performs a **direct COCO evaluation** of the DCCAN detector on a chosen VOC split.  
It **does not** export CSVs or annotated figures in its current form.
"""

!pip install -q torchmetrics

# Evaluation for DCCAN Faster R-CNN 20 epochs
import sys
from pathlib import Path

import cv2
import numpy as np
import pandas as pd
import torch
from PIL import Image
from tqdm import tqdm
from torch.utils.data import DataLoader
from torchvision.transforms import ToTensor
from torchvision.ops import nms, box_iou
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.models.detection.faster_rcnn import (
    FastRCNNPredictor, FasterRCNN_ResNet50_FPN_Weights
)
from torchmetrics.detection.mean_ap import MeanAveragePrecision

# CONFIG
PROJECT_ROOT = Path("/content/drive/MyDrive/sonar-object-detection")
DATA_ROOT    = PROJECT_ROOT / "data" / "line2voc"
IMAGE_SET    = "val"
CKPT_PATH    = PROJECT_ROOT / "checkpoints" / "dccan_sonar_fasterrcnn_stabilized_20e.pth"

OUT_DIR      = PROJECT_ROOT / "outputs" / "eval_dccan_allinone"
OUT_DIR.mkdir(parents=True, exist_ok=True)

RAW_CSV = OUT_DIR / f"preds_dccan_20epoch_{IMAGE_SET}_RAW.csv"
VIS_CSV = OUT_DIR / f"preds_dccan_20epoch_{IMAGE_SET}_VIS.csv"
VIS_DIR = OUT_DIR / f"vis_{IMAGE_SET}"
VIS_DIR.mkdir(parents=True, exist_ok=True)

CLASS_NAME   = {1: "object", 2: "shadow"}
CLASS_COLORS = {1: (0, 255, 0), 2: (0, 0, 255)}
MAX_DETS_PER_IMAGE = 300

# VIS-only knobs
NMS_IOU_CLASSWISE    = 0.50
MERGE_IOU_CLASSWISE  = 0.60
CROSS_CLASS_IOU      = 0.85
CONF_THRESH_VIS      = 0.50
DO_GREEDY_FUSION     = True
DO_CROSS_CLASS_SUPPR = True

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
torch.backends.cudnn.benchmark = True
print("Device:", device)

# Dataset wrapper
sys.path.append(str(PROJECT_ROOT / "scripts"))
from voc_dataset import VOCDataset

# Build model
def build_model(checkpoint_path: Path):
    weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT
    model = fasterrcnn_resnet50_fpn(weights=weights)
    in_feats = model.roi_heads.box_predictor.cls_score.in_features
    model.roi_heads.box_predictor = FastRCNNPredictor(in_feats, num_classes=3)
    model.roi_heads.score_thresh = 0.0
    model.roi_heads.nms_thresh = 0.5
    model.roi_heads.detections_per_img = MAX_DETS_PER_IMAGE

    state = torch.load(str(checkpoint_path), map_location=device)
    if isinstance(state, dict) and "model" in state:
        state = state["model"]
    model.load_state_dict(state)
    return model.to(device).eval()

# visual dedup helpers
def _classwise_nms(boxes, scores, labels, iou_thr):
    if boxes.numel() == 0:
        return boxes, scores, labels
    keep_all = []
    for c in labels.unique().tolist():
        idx = torch.nonzero(labels == c, as_tuple=False).squeeze(1)
        if idx.numel() == 0:
            continue
        kept = nms(boxes[idx], scores[idx], iou_thr)
        keep_all.append(idx[kept])
    if not keep_all:
        return boxes[:0], scores[:0], labels[:0]
    keep = torch.cat(keep_all, dim=0)
    return boxes[keep], scores[keep], labels[keep]

def _greedy_merge_per_class(boxes, scores, labels, iou_thr):
    if boxes.numel() == 0:
        return boxes, scores, labels
    out_b, out_s, out_l = [], [], []
    for c in labels.unique().tolist():
        idx = torch.nonzero(labels == c, as_tuple=False).squeeze(1)
        if idx.numel() == 0:
            continue
        B, S = boxes[idx].clone(), scores[idx].clone()
        order = torch.argsort(S, descending=True)
        used = torch.zeros(order.numel(), dtype=torch.bool)
        for r, j in enumerate(order):
            if used[r]: continue
            base = B[j].unsqueeze(0)
            ious = box_iou(base, B[order])[0]
            mask = (ious >= iou_thr) & (~used)
            members = order[mask]
            w = S[members]
            fused = (B[members] * w[:, None]).sum(0) / (w.sum() + 1e-8)
            out_b.append(fused); out_s.append(S[members].max()); out_l.append(torch.tensor(c))
            used |= mask
    return torch.stack(out_b), torch.stack(out_s), torch.stack(out_l)

def _cross_class_suppress(boxes, scores, labels, iou_thr):
    if boxes.numel() == 0: return boxes, scores, labels
    order = torch.argsort(scores, descending=True)
    keep = torch.ones(order.numel(), dtype=torch.bool)
    for i in range(order.numel()):
        if not keep[i]: continue
        bi, li = boxes[order[i]].unsqueeze(0), labels[order[i]]
        rest = order[i+1:]
        if rest.numel() == 0: continue
        ious = box_iou(bi, boxes[rest])[0]
        clash = (ious >= iou_thr) & (labels[rest] != li)
        keep[i+1:][clash] = False
    final = order[keep]
    return boxes[final], scores[final], labels[final]

def _enforce_topk(boxes, scores, labels, k):
    if scores.numel() <= k: return boxes, scores, labels
    idx = torch.argsort(scores, descending=True)[:k]
    return boxes[idx], scores[idx], labels[idx]

# eval functions
def coco_eval_from_model(model, dataset_root, image_set, batch_size=4, num_workers=2):
    ds = VOCDataset(str(dataset_root), image_set=image_set, transforms=ToTensor())
    dl = DataLoader(ds, batch_size=batch_size, shuffle=False,
                    collate_fn=lambda x: tuple(zip(*x)), num_workers=num_workers, pin_memory=True)
    metric = MeanAveragePrecision(class_metrics=True)

    for imgs, targets in dl:
        imgs = [img.to(device) for img in imgs]
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]
        with torch.no_grad(): outs = model(imgs)
        preds, gts = [], []
        for o, t in zip(outs, targets):
            preds.append({"boxes": o["boxes"].cpu(), "scores": o["scores"].cpu(), "labels": o["labels"].cpu()})
            gts.append({"boxes": t["boxes"].cpu(), "labels": t["labels"].cpu()})
        metric.update(preds, gts)

    res = metric.compute()
    print("\n-- Direct COCO (raw model outputs) --")
    for k, v in res.items():
        if isinstance(v, torch.Tensor) and v.ndim == 0:
            print(f"{k}: {float(v):.4f}")
    return res

# main
if __name__ == "__main__":
    model = build_model(CKPT_PATH)
    direct_res = coco_eval_from_model(model, DATA_ROOT, IMAGE_SET, batch_size=8, num_workers=2)

"""## **Visualize DCCAN Predictions: Validation Split (Raw Target)**

This cell renders **qualitative screenshots** of the **DCCAN-trained Faster R-CNN (ResNet-50 FPN, 3 classes)** on the `line2voc` **validation** split (raw target domain). It rebuilds the detector exactly like training, loads the DCCAN checkpoint, and overlays predicted boxes/scores. These figures are **for human inspection only** and are **not** used for metrics.
"""

# VISUALIZE PREDICTIONS: DCCAN 20 epochs with raw target split

import sys
import torch
import cv2
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from torchvision.transforms import ToTensor
from torchvision.ops import nms
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.models.detection.faster_rcnn import (
    FastRCNNPredictor,
    FasterRCNN_ResNet50_FPN_Weights,
)


# Config
CONF_THRESH   = 0.50        # draw boxes with score >= 0.50
IOU_THRESH    = 0.65        # IoU used by the optional extra NMS I implemented
NMS_MODE      = "classwise"  # Can be "agnostic" or "classwise",but I chose classwise
NUM_IMAGES    = 5

# Paths
PROJECT_ROOT  = Path("/content/drive/MyDrive/sonar-object-detection")
DATASET_ROOT  = str(PROJECT_ROOT / "data" / "line2voc")
CKPT          = str(PROJECT_ROOT / "checkpoints" / "dccan_sonar_fasterrcnn_stabilized_20e.pth")

# Labels & colors, consistent accross scripts
LABEL_NAMES   = {1: "object", 2: "shadow"}
LABEL_COLORS  = {1: (0, 255, 0), 2: (0, 0, 255)}  # OpenCV BGR

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# Dataset wrapper
sys.path.append(str(PROJECT_ROOT / "scripts"))
from voc_dataset import VOCDataset


# Helper
def apply_extra_nms(boxes, scores, labels, iou_thresh=0.5, mode="agnostic"):
    if boxes.numel() == 0:
        return torch.empty((0,), dtype=torch.long)

    if mode == "classwise":
        keep_idxs = []
        for cls in torch.unique(labels):
            mask = labels == cls
            if mask.any():
                kept = nms(boxes[mask], scores[mask], iou_thresh)
                keep_idxs.append(torch.where(mask)[0][kept])
        return torch.cat(keep_idxs) if keep_idxs else torch.empty((0,), dtype=torch.long)

    # default: class-agnostic
    return nms(boxes, scores, iou_thresh)

# Model
# Use DEFAULT so the internal transform matches training.
weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT
model = fasterrcnn_resnet50_fpn(weights=weights)

# Swap head to 3 classes: bg=0, object=1, shadow=2
in_features = model.roi_heads.box_predictor.cls_score.in_features
model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes=3)

# Load DCCAN detector weights
state = torch.load(CKPT, map_location=device)
if isinstance(state, dict) and "model" in state:
    state = state["model"]
model.load_state_dict(state)

# Hygiene for VIS ONLY to prevent hidden pruning inside ROIHeads
model.roi_heads.score_thresh = 0.0          # don't drop boxes internally
model.roi_heads.nms_thresh = 0.5            # to make default explicit
model.roi_heads.detections_per_img = 300    # to avoid silent truncation

model.to(device).eval()


# Visualize a few val images
def visualize(model, dataset_root, num_images, conf_thresh, iou_thresh, nms_mode):
    ds = VOCDataset(dataset_root, image_set="val", transforms=ToTensor())

    for idx in range(min(num_images, len(ds))):
        img, _ = ds[idx]
        x = img.unsqueeze(0).to(device)

        with torch.no_grad():
            out = model(x)[0]

        # dedup just for visuals
        keep = apply_extra_nms(out["boxes"], out["scores"], out["labels"],
                               iou_thresh=iou_thresh, mode=nms_mode)
        boxes  = out["boxes"][keep]
        scores = out["scores"][keep]
        labels = out["labels"][keep]

        # Convert to OpenCV BGR for drawing
        im = cv2.cvtColor(
            (img.mul(255).byte().permute(1, 2, 0).cpu().numpy()),
            cv2.COLOR_RGB2BGR
        )

        shown = 0
        for b, l, s in zip(boxes, labels, scores):
            s = float(s)
            if s < conf_thresh:
                continue
            x1, y1, x2, y2 = b.int().tolist()
            color = LABEL_COLORS.get(int(l), (255, 255, 255))
            name  = LABEL_NAMES.get(int(l), "unknown")

            cv2.rectangle(im, (x1, y1), (x2, y2), color, 2)
            cv2.putText(im, f"{name} {s:.2f}", (x1, max(10, y1 - 6)),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
            shown += 1

        print(
            f"DCCAN | Image {idx+1}: drew {shown} boxes (score ≥ {conf_thresh:.2f}) "
            f"after {nms_mode} NMS at IoU={iou_thresh:.2f}"
        )

        # Show figure
        plt.figure(figsize=(10, 6))
        plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))
        plt.axis("off")
        plt.title(f"DCCAN (20e) Image {idx+1} | {nms_mode} NMS IoU={iou_thresh}")
        plt.tight_layout(pad=0)
        plt.show()


# Run
visualize(model, DATASET_ROOT, NUM_IMAGES, CONF_THRESH, IOU_THRESH, NMS_MODE)

"""# **DCCAN 25 Epoch**"""

# DCCAN training with 30epoch
import sys
from pathlib import Path

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.amp import autocast, GradScaler
from torch.utils.data import DataLoader

import torchvision.transforms.v2 as T2
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.models.detection.faster_rcnn import (
    FastRCNNPredictor,
    FasterRCNN_ResNet50_FPN_Weights,
)

# THUML discriminator blocks
from tllib.modules.domain_discriminator import DomainDiscriminator

import pandas as pd
from tqdm import tqdm
import math
import random

# Basic config
device        = torch.device("cuda" if torch.cuda.is_available() else "cpu")
BATCH_SIZE    = 12
EPOCHS        = 25
FREEZE_EPOCHS = 2              # freeze backbone the first 2 epochs
NUM_CLASSES   = 3              # background=0, object=1, shadow=2
F_DIM         = 256
SEED          = 1337

# Adaptation strengths
DANN_LAMBDA_MAX = 0.20
CDAN_LAMBDA_MAX = 0.30
RPN_LAMBDA_MAX  = 0.10

# Temperature for CDAN conditioning and confidence gating schedule
CDAN_TEMP        = 0.6         # logits temperature
CONF_THR_START   = 0.40        # only use samples with max prob >= thr early
CONF_THR_END     = 0.20        # relax over time

# Stability
MAX_NORM_GRAD    = 5.0         # gradient clipping to help precision by avoiding noisy updates

torch.backends.cudnn.benchmark = True

# Reproducibility
def set_seed(s=SEED):
    random.seed(s)
    torch.manual_seed(s)
    torch.cuda.manual_seed_all(s)
    torch.use_deterministic_algorithms(False)

set_seed()

# Dataset + collate
sys.path.append("/content/drive/MyDrive/sonar-object-detection/scripts")
from voc_dataset import VOCDataset as CustomVOCDataset

def collate_fn(batch):
    # returns (list_of_images, list_of_targets)
    return list(zip(*batch))

# keep transforms minimal
transform = T2.Compose([
    T2.ToImage(),
    T2.ToDtype(torch.float32, scale=True),
])

source_dataset = CustomVOCDataset(
    "/content/drive/MyDrive/sonar-object-detection/data/line2voc_preprocessed",
    "train_source_cleaned",
    transforms=transform,
)
target_dataset = CustomVOCDataset(
    "/content/drive/MyDrive/sonar-object-detection/data/line2voc",
    "train_target_cleaned",
    transforms=transform,
)

source_loader = DataLoader(source_dataset, BATCH_SIZE, True, num_workers=2,
                           collate_fn=collate_fn, pin_memory=True, drop_last=True)
target_loader = DataLoader(target_dataset, BATCH_SIZE, True, num_workers=2,
                           collate_fn=collate_fn, pin_memory=True, drop_last=True)


# Detector: Faster R-CNN
weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT
model = fasterrcnn_resnet50_fpn(weights=weights)
in_features = model.roi_heads.box_predictor.cls_score.in_features
model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES)
model.to(device)

# Proxy head to get logits from pooled features for CDAN
proxy_cls = nn.Linear(F_DIM, NUM_CLASSES).to(device)

# Domain discriminators
# DANN head: global pooled features
disc_dann = DomainDiscriminator(F_DIM, 1024).to(device)
# CDAN head: flattened outer-product
disc_cdan = DomainDiscriminator(F_DIM * NUM_CLASSES, 1024).to(device)

# RPN-level alignment
class RPNDomainHead(nn.Module):
    def __init__(self, in_ch=F_DIM):
        super().__init__()
        self.conv1 = nn.Conv2d(in_ch, 64, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(64, 1, kernel_size=1)
        self.act   = nn.ReLU(inplace=True)
    def forward(self, x):
        h = self.act(self.conv1(x))
        return self.conv2(h)

rpn_domain_head = RPNDomainHead().to(device)

# Gradient Reversal Layer with dynamic coeff
class _GradReverse(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, coeff):
        ctx.coeff = coeff
        return x.view_as(x)
    @staticmethod
    def backward(ctx, g):
        return -ctx.coeff * g, None

class GRL(nn.Module):
    def __init__(self, coeff=0.0):
        super().__init__()
        self.coeff = float(coeff)
    def set_coeff(self, c: float):
        self.coeff = float(c)
    def forward(self, x):
        return _GradReverse.apply(x, self.coeff)

grl_global = GRL(0.0).to(device)  # for DANN
grl_cdan   = GRL(0.0).to(device)  # for CDAN mapping
grl_rpn    = GRL(0.0).to(device)  # for RPN-level alignment

# Losses
bce_logits = nn.BCEWithLogitsLoss(reduction="mean")

def dann_loss(feat_s, feat_t):
    feats = torch.cat([feat_s, feat_t], dim=0)
    feats = grl_global(feats)
    logits = disc_dann(feats)
    labels = torch.cat([
        torch.ones((feat_s.size(0), 1), device=feats.device),
        torch.zeros((feat_t.size(0), 1), device=feats.device)
    ], dim=0)
    loss = bce_logits(logits, labels)
    with torch.no_grad():
        pred = (torch.sigmoid(logits) >= 0.5).float()
        acc = (pred.eq(labels)).float().mean().item()
    return loss, acc

def l2_normalize_rows(x, eps=1e-6):

    return x / (x.norm(p=2, dim=1, keepdim=True) + eps)

def cdan_loss(g_s, f_s, g_t, f_t, temp=CDAN_TEMP, conf_thr=0.3):

    # softmax with temperature, detach to avoid pushing classifier by domain loss
    p_s = F.softmax(g_s / temp, dim=1).detach()
    p_t = F.softmax(g_t / temp, dim=1).detach()

    # L2 normalize
    f_s_n = l2_normalize_rows(f_s)
    f_t_n = l2_normalize_rows(f_t)
    p_s_n = l2_normalize_rows(p_s)
    p_t_n = l2_normalize_rows(p_t)

    # outer product per sample
    def outer_flat(f, p):
        # f: [B, 256], p: [B, C]
        B, D = f.shape
        h = torch.bmm(f.unsqueeze(2), p.unsqueeze(1))
        return h.reshape(B, D * p.shape[1])

    h_s = outer_flat(f_s_n, p_s_n)
    h_t = outer_flat(f_t_n, p_t_n)
    h   = torch.cat([h_s, h_t], dim=0)
    h   = grl_cdan(h)

    logits = disc_cdan(h)
    labels = torch.cat([
        torch.ones((g_s.size(0), 1), device=logits.device),
        torch.zeros((g_t.size(0), 1), device=logits.device)
    ], dim=0)

    # confidence gating
    conf = torch.cat([p_s.max(dim=1).values, p_t.max(dim=1).values], dim=0)
    weights = (conf >= conf_thr).float().unsqueeze(1)
    denom = weights.sum().clamp(min=1.0)
    loss = F.binary_cross_entropy_with_logits(logits, labels, weight=weights, reduction='sum') / denom

    with torch.no_grad():
        pred = (torch.sigmoid(logits) >= 0.5).float()
        acc = (pred.eq(labels)).float().mean().item()
        kept = int(denom.item())
    return loss, acc, kept

def rpn_domain_loss(p3_s, p3_t):


    x = torch.cat([p3_s, p3_t], dim=0)
    x = grl_rpn(x)
    logits_map = rpn_domain_head(x)
    # global average pool to per-sample logits
    logits = logits_map.mean(dim=[2, 3])
    labels = torch.cat([
        torch.ones((p3_s.size(0), 1), device=logits.device),
        torch.zeros((p3_t.size(0), 1), device=logits.device)
    ], dim=0)
    loss = bce_logits(logits, labels)
    with torch.no_grad():
        pred = (torch.sigmoid(logits) >= 0.5).float()
        acc = (pred.eq(labels)).float().mean().item()
    return loss, acc


# Optimizer with parameter groups
# Detector & proxy head
detector_params = list(model.parameters()) + list(proxy_cls.parameters())
# Domain heads
disc_params = list(disc_dann.parameters()) + list(disc_cdan.parameters()) + list(rpn_domain_head.parameters())

optimizer = torch.optim.SGD([
    {"params": detector_params, "lr": 1.5e-3, "momentum": 0.9, "weight_decay": 5e-4},
    {"params": disc_params,     "lr": 4.5e-4, "momentum": 0.9, "weight_decay": 5e-4},
], lr=1.5e-3, momentum=0.9, weight_decay=5e-4)

scaler = GradScaler()


# Utilities: schedule and freezing
def grl_schedule(progress):

    base = 2.0 / (1.0 + math.exp(-10.0 * progress)) - 1.0
    return base

def conf_thr_schedule(progress):
    # linear from CONF_THR_START to CONF_THR_END over first half, then hold
    if progress <= 0.5:
        alpha = progress / 0.5
        return CONF_THR_START + (CONF_THR_END - CONF_THR_START) * alpha
    else:
        return CONF_THR_END

def set_backbone_trainable(module, flag: bool):
    for p in module.parameters():
        p.requires_grad = flag


# Training
model.train(); proxy_cls.train(); disc_dann.train(); disc_cdan.train(); rpn_domain_head.train()

total_steps = min(len(source_loader), len(target_loader)) * EPOCHS
global_step = 0

log_rows = []

for epoch in range(1, EPOCHS + 1):
    # backbone warmup: first FREEZE_EPOCHS epochs -> train only detector heads without adaptation
    backbone_trainable = (epoch > FREEZE_EPOCHS)
    set_backbone_trainable(model.backbone, backbone_trainable)

    sum_det = sum_dann = sum_cdan = sum_rpn = 0.0
    sum_dacc = sum_cacc = sum_racc = 0.0
    sum_cdan_kept = 0
    nb = 0

    loop = tqdm(
        zip(source_loader, target_loader),
        total=min(len(source_loader), len(target_loader)),
        desc=f"Epoch {epoch}/{EPOCHS}"
    )

    for (src_imgs, src_tgts), (tgt_imgs, _) in loop:
        nb += 1
        global_step += 1
        progress = global_step / total_steps

        # GRL coefficients
        ramp = grl_schedule(progress)
        grl_global.set_coeff( (DANN_LAMBDA_MAX if backbone_trainable else 0.0) * ramp )
        grl_cdan.set_coeff(   (CDAN_LAMBDA_MAX if backbone_trainable else 0.0) * ramp )
        grl_rpn.set_coeff(    (RPN_LAMBDA_MAX  if backbone_trainable else 0.0) * ramp )
        conf_thr = conf_thr_schedule(progress)

        # to device
        src_imgs = [img.to(device, non_blocking=True) for img in src_imgs]
        tgt_imgs = [img.to(device, non_blocking=True) for img in tgt_imgs]
        src_tgts = [{k: v.to(device, non_blocking=True) for k, v in t.items()} for t in src_tgts]

        with autocast(device_type="cuda", enabled=(device.type == "cuda")):
            # (1) supervised detection loss on source
            det_loss = sum(model(src_imgs, src_tgts).values())

            # (2) Extract features using the SAME preprocessing as the detector path
            s_img_list, _ = model.transform(src_imgs, None)
            t_img_list, _ = model.transform(tgt_imgs, None)

            p3_s = model.backbone(s_img_list.tensors)["0"]
            p3_t = model.backbone(t_img_list.tensors)["0"]

            f_s_pool = F.adaptive_avg_pool2d(p3_s, 1).reshape(p3_s.size(0), -1)
            f_t_pool = F.adaptive_avg_pool2d(p3_t, 1).reshape(p3_t.size(0), -1)

            # (3) Proxy logits for CDAN conditioning
            g_s = proxy_cls(f_s_pool)  # [Bs, C]
            g_t = proxy_cls(f_t_pool)  # [Bt, C]

            # (4) Domain losses
            if backbone_trainable:
                dann_l, dann_acc = dann_loss(f_s_pool, f_t_pool)
                cdan_l, cdan_acc, cdan_kept = cdan_loss(g_s, f_s_pool, g_t, f_t_pool, temp=CDAN_TEMP, conf_thr=conf_thr)
                rpn_l,  rpn_acc  = rpn_domain_loss(p3_s, p3_t)
            else:
                # no adaptation pressure during warmup
                dann_l = cdan_l = rpn_l = torch.tensor(0.0, device=device)
                dann_acc = cdan_acc = rpn_acc = 0.0
                cdan_kept = 0

            # (5) Total loss
            loss = det_loss + dann_l + cdan_l + rpn_l

        optimizer.zero_grad(set_to_none=True)
        scaler.scale(loss).backward()
        # stability: clip grads to curb noisy updates and improve precision
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(detector_params, MAX_NORM_GRAD)
        torch.nn.utils.clip_grad_norm_(disc_params,     MAX_NORM_GRAD)
        scaler.step(optimizer)
        scaler.update()

        # Accumulate logs
        sum_det  += det_loss.item()
        sum_dann += float(dann_l.item())
        sum_cdan += float(cdan_l.item())
        sum_rpn  += float(rpn_l.item())
        sum_dacc += dann_acc
        sum_cacc += cdan_acc
        sum_racc += rpn_acc
        sum_cdan_kept += cdan_kept

        loop.set_postfix({
            "Det": f"{det_loss.item():.3f}",
            "DANN": f"{dann_l.item():.3f}",
            "CDAN": f"{cdan_l.item():.3f}",
            "RPN": f"{rpn_l.item():.3f}",
            "accD": f"{dann_acc:.2f}",
            "accC": f"{cdan_acc:.2f}",
            "accR": f"{rpn_acc:.2f}",
            "conf_thr": f"{conf_thr:.2f}",
            "grlD": f"{grl_global.coeff:.2f}",
            "grlC": f"{grl_cdan.coeff:.2f}",
            "grlR": f"{grl_rpn.coeff:.2f}",
        })

    # epoch summary
    n = max(nb, 1)
    print(f"Epoch {epoch}: Det {sum_det:.2f}  DANN {sum_dann:.2f} (acc {sum_dacc/n:.2f})  "
          f"CDAN {sum_cdan:.2f} (acc {sum_cacc/n:.2f}, kept {int(sum_cdan_kept/n)})  "
          f"RPN {sum_rpn:.2f} (acc {sum_racc/n:.2f})")

    log_rows.append({
        "epoch": epoch,
        "det_loss": sum_det,
        "dann_loss": sum_dann,
        "cdan_loss": sum_cdan,
        "rpn_loss": sum_rpn,
        "dann_acc": sum_dacc / n,
        "cdan_acc": sum_cacc / n,
        "rpn_acc": sum_racc / n,
    })


# Save artifacts
ckpt_dir = Path("/content/drive/MyDrive/sonar-object-detection/checkpoints")
out_dir  = Path("/content/drive/MyDrive/sonar-object-detection/outputs")
ckpt_dir.mkdir(exist_ok=True, parents=True)
out_dir.mkdir(exist_ok=True, parents=True)

torch.save({
    "model":     model.state_dict(),
    "proxy_cls": proxy_cls.state_dict(),
    "disc_dann": disc_dann.state_dict(),
    "disc_cdan": disc_cdan.state_dict(),
    "rpn_head":  rpn_domain_head.state_dict(),
    "cfg": {
        "EPOCHS": EPOCHS,
        "FREEZE_EPOCHS": FREEZE_EPOCHS,
        "DANN_LAMBDA_MAX": DANN_LAMBDA_MAX,
        "CDAN_LAMBDA_MAX": CDAN_LAMBDA_MAX,
        "RPN_LAMBDA_MAX": RPN_LAMBDA_MAX,
        "CDAN_TEMP": CDAN_TEMP,
    }
}, ckpt_dir / "dccan_sonar_fasterrcnn_stabilized_25e.pth")

pd.DataFrame(log_rows).to_csv(out_dir / "dccan_stabilized_loss_curve_25e.csv", index=False)
print("Saved stabilized DCCAN checkpoint and training log.")

# BATCH INFERENCE: DCCAN Faster R-CNN 25 epochs on RAW sonar
# Exports TWO CSVs:
#   1) preds_dccan_25epoch_RAW.csv
#   2) preds_dccan_25epoch_VIS.csv
import os, cv2, torch, pandas as pd, numpy as np
from PIL import Image
from pathlib import Path
from tqdm import tqdm

from torchvision.transforms import ToTensor
from torchvision.ops import nms, box_iou
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.models.detection.faster_rcnn import (
    FastRCNNPredictor, FasterRCNN_ResNet50_FPN_Weights
)

# Paths & device
PROJECT_ROOT = Path("/content/drive/MyDrive/sonar-object-detection")
IMG_DIR      = PROJECT_ROOT / "data" / "line2voc" / "JPEGImages"
CKPT_PATH    = PROJECT_ROOT / "checkpoints" / "dccan_sonar_fasterrcnn_stabilized_25e.pth"

VIS_DIR     = PROJECT_ROOT / "outputs" / "vis_dccan_25epoch"
OUT_DIR     = PROJECT_ROOT / "outputs"
RAW_CSV     = OUT_DIR / "preds_dccan_25epoch_RAW.csv"
VIS_CSV     = OUT_DIR / "preds_dccan_25epoch_VIS.csv"

VIS_DIR.mkdir(parents=True, exist_ok=True)
OUT_DIR.mkdir(parents=True, exist_ok=True)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

# knobs
# RAW should mirror evaluation settings
RAW_MAX_DETS          = 300
RAW_NMS_THR           = 0.50
# VIS pipeline
SCORE_THRESH          = 0.50
NMS_IOU_CLASSWISE     = 0.50
MERGE_IOU_CLASSWISE   = 0.60
CROSS_CLASS_IOU       = 0.95
DO_GREEDY_FUSION      = True
DO_CROSS_CLASS_SUPPR  = True
VIS_MAX_DETS          = 100
# Small-box filter
MIN_W, MIN_H, MIN_AREA = 8, 8, 64
# Optional TTA
DO_TTA_FLIP = False

CLASS_NAMES  = {1: "object", 2: "shadow"}
CLASS_COLORS = {1: (0, 255, 0), 2: (0, 0, 255)}
CLASS_THRESH = {1: 0.60, 2: 0.70}

# Model
weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT
model = fasterrcnn_resnet50_fpn(weights=weights)
in_features = model.roi_heads.box_predictor.cls_score.in_features
model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes=3)

# Load checkpoint + mirror any training resize cfg
state = torch.load(CKPT_PATH, map_location=device)
cfg = {}
if isinstance(state, dict) and "model" in state:
    cfg = state.get("cfg", {})
    state = state["model"]
model.load_state_dict(state)

# Note to whom will be using my code after submission:
# If you trained with smaller resize, mirror it here
if cfg.get("USE_SMALLER_RESIZE", False):
    ms = tuple(cfg.get("MIN_SIZE_TRAIN", (800,)))
    mx = int(cfg.get("MAX_SIZE_TRAIN", 1333))
    model.transform.min_size = ms
    model.transform.max_size = mx
    print(f"[info] mirrored train resize -> min={ms}, max={mx}")

model.to(device).eval()
to_tensor = ToTensor()

# helpers
def _clip_and_validate(boxes, W, H):
    if boxes.numel() == 0:
        return boxes, torch.zeros((0,), dtype=torch.bool)
    boxes = boxes.clone()
    boxes[:, [0, 2]] = boxes[:, [0, 2]].clamp(0, W - 1)
    boxes[:, [1, 3]] = boxes[:, [1, 3]].clamp(0, H - 1)
    keep = (boxes[:, 2] > boxes[:, 0]) & (boxes[:, 3] > boxes[:, 1])
    return boxes[keep], keep

def _filter_small(boxes, scores, labels):
    if boxes.numel() == 0:
        return boxes, scores, labels
    wh = boxes[:, 2:4] - boxes[:, 0:2]
    keep = (wh[:, 0] >= MIN_W) & (wh[:, 1] >= MIN_H) & (wh.prod(dim=1) >= MIN_AREA)
    return boxes[keep], scores[keep], labels[keep]

def _classwise_nms(boxes, scores, labels, iou_thr):
    if boxes.numel() == 0:
        return boxes, scores, labels
    keep_all = []
    for c in labels.unique().tolist():
        idx = torch.nonzero(labels == c, as_tuple=False).squeeze(1)
        if idx.numel() == 0:
            continue
        kept = nms(boxes[idx], scores[idx], iou_thr)
        keep_all.append(idx[kept])
    if not keep_all:
        return boxes[:0], scores[:0], labels[:0]
    keep = torch.cat(keep_all, dim=0)
    return boxes[keep], scores[keep], labels[keep]

def _greedy_merge_per_class(boxes, scores, labels, iou_thr):
    if boxes.numel() == 0:
        return boxes, scores, labels
    out_b, out_s, out_l = [], [], []
    for c in labels.unique().tolist():
        idx = torch.nonzero(labels == c, as_tuple=False).squeeze(1)
        if idx.numel() == 0:
            continue
        B, S = boxes[idx].clone(), scores[idx].clone()
        order = torch.argsort(S, descending=True)
        used = torch.zeros(order.numel(), dtype=torch.bool)
        for r, j in enumerate(order):
            if used[r]:
                continue
            base = B[j].unsqueeze(0)
            ious = box_iou(base, B[order])[0]
            mask = (ious >= iou_thr) & (~used)
            members = order[mask]
            w = S[members]
            fused = (B[members] * w[:, None]).sum(0) / (w.sum() + 1e-8)
            out_b.append(fused); out_s.append(S[members].max()); out_l.append(torch.tensor(c, dtype=labels.dtype))
            used |= mask
    return torch.stack(out_b), torch.stack(out_s), torch.stack(out_l)

def _cross_class_suppress(boxes, scores, labels, iou_thr):
    if boxes.numel() == 0:
        return boxes, scores, labels
    order = torch.argsort(scores, descending=True)
    keep = torch.ones(order.numel(), dtype=torch.bool)
    for i in range(order.numel()):
        if not keep[i]:
            continue
        bi, li = boxes[order[i]].unsqueeze(0), labels[order[i]]
        rest = order[i+1:]
        if rest.numel() == 0:
            continue
        ious = box_iou(bi, boxes[rest])[0]
        clash = (ious >= iou_thr) & (labels[rest] != li)
        keep[i+1:][clash] = False
    final = order[keep]
    return boxes[final], scores[final], labels[final]

def _enforce_topk(boxes, scores, labels, k):
    if scores.numel() <= k:
        return boxes, scores, labels
    idx = torch.argsort(scores, descending=True)[:k]
    return boxes[idx], scores[idx], labels[idx]

def _per_class_threshold(boxes, scores, labels, thresh_map, default_thr=0.5):
    if scores.numel() == 0:
        return boxes, scores, labels
    keep = torch.tensor(
        [s >= thresh_map.get(int(l), default_thr) for s, l in zip(scores, labels)],
        dtype=torch.bool
    )
    return boxes[keep], scores[keep], labels[keep]

def _run_model(img_pil):
    x = to_tensor(img_pil).to(device)
    with torch.no_grad():
        out_main = model([x])[0]

    boxes  = out_main.get("boxes",  torch.empty((0, 4), device=device))
    scores = out_main.get("scores", torch.empty((0,),   device=device))
    labels = out_main.get("labels", torch.empty((0,),   device=device, dtype=torch.long))

    if DO_TTA_FLIP:
        W, H = img_pil.size
        x_flip = torch.flip(x, dims=[2])
        with torch.no_grad():
            out_flip = model([x_flip])[0]
        b2  = out_flip.get("boxes",  torch.empty((0, 4), device=device)).clone()
        s2  = out_flip.get("scores", torch.empty((0,),   device=device))
        l2  = out_flip.get("labels", torch.empty((0,),   device=device, dtype=torch.long))
        if b2.numel() > 0:
            x1 = b2[:, 0].clone(); y1 = b2[:, 1].clone()
            x2 = b2[:, 2].clone(); y2 = b2[:, 3].clone()
            b2[:, 0] = (W - x2); b2[:, 2] = (W - x1)
            b2[:, 1] = y1;       b2[:, 3] = y2
        boxes  = torch.cat([boxes, b2], dim=0)
        scores = torch.cat([scores, s2],  dim=0)
        labels = torch.cat([labels, l2],  dim=0)

    return boxes.detach().cpu(), scores.detach().cpu(), labels.detach().cpu()

# Dual-mode export
def export_preds(model, image_dir, out_path, mode="RAW"):
    assert mode in {"RAW", "VIS"}
    image_dir = Path(image_dir)
    paths = sorted(
        list(image_dir.glob("*.jpg")) + list(image_dir.glob("*.JPG")) +
        list(image_dir.glob("*.png")) + list(image_dir.glob("*.PNG"))
    )

    # set per-mode detector caps for consistency
    if mode == "RAW":
        model.roi_heads.score_thresh = 0.0
        model.roi_heads.nms_thresh   = RAW_NMS_THR
        model.roi_heads.detections_per_img = RAW_MAX_DETS
    else:
        model.roi_heads.score_thresh = 0.0
        model.roi_heads.nms_thresh   = RAW_NMS_THR
        model.roi_heads.detections_per_img = VIS_MAX_DETS

    rows = []
    for p in tqdm(paths, desc=f"DCCAN export ({mode})"):
        img = Image.open(p).convert("RGB")
        W, H = img.size

        boxes, scores, labels = _run_model(img)

        # Always clip to image and drop invalids
        boxes, keep_mask = _clip_and_validate(boxes, W, H)
        scores = scores[keep_mask]; labels = labels[keep_mask]

        if mode == "RAW":
            # RAW: NO small-box filter, NO extra NMS — just keep topK
            boxes, scores, labels = _enforce_topk(boxes, scores, labels, RAW_MAX_DETS)
        else:
            # VIS-only refinements
            boxes, scores, labels = _filter_small(boxes, scores, labels)
            boxes, scores, labels = _classwise_nms(boxes, scores, labels, NMS_IOU_CLASSWISE)
            if DO_GREEDY_FUSION and boxes.numel() > 0:
                boxes, scores, labels = _greedy_merge_per_class(boxes, scores, labels, MERGE_IOU_CLASSWISE)
            if DO_CROSS_CLASS_SUPPR and boxes.numel() > 0:
                boxes, scores, labels = _cross_class_suppress(boxes, scores, labels, CROSS_CLASS_IOU)
            boxes, scores, labels = _enforce_topk(boxes, scores, labels, VIS_MAX_DETS)
            boxes, scores, labels = _per_class_threshold(boxes, scores, labels, CLASS_THRESH, default_thr=SCORE_THRESH)

            # draw
            im = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)
            for b, s, l in zip(boxes, scores, labels):
                x1, y1, x2, y2 = map(int, b.tolist())
                color = CLASS_COLORS.get(int(l), (255, 255, 255))
                name  = CLASS_NAMES.get(int(l), "unknown")
                cv2.rectangle(im, (x1, y1), (x2, y2), color, 2)
                cv2.putText(im, f"{name} ({float(s):.2f})", (x1, y1 - 5),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
            cv2.imwrite(str(VIS_DIR / p.name), im)

        # rows
        for b, s, l in zip(boxes, scores, labels):
            x1, y1, x2, y2 = map(float, b.tolist())
            rows.append({
                "filename": p.name,
                "class_id": int(l),
                "class_name": CLASS_NAMES.get(int(l), "unknown"),
                "score": float(s),
                "xmin": x1, "ymin": y1, "xmax": x2, "ymax": y2,
            })

    pd.DataFrame(rows).to_csv(out_path, index=False)
    print(f"{mode} predictions -> {out_path}")
    if mode == "VIS":
        print(f"Annotated images -> {VIS_DIR}")

# Run both exports
if __name__ == "__main__":
    export_preds(model, IMG_DIR, RAW_CSV, mode="RAW")
    export_preds(model, IMG_DIR, VIS_CSV, mode="VIS")

# Evaluation for DCCAN Faster R-CNN 25 epochs
import sys
from pathlib import Path

import cv2
import numpy as np
import pandas as pd
import torch
from PIL import Image
from tqdm import tqdm
from torch.utils.data import DataLoader
from torchvision.transforms import ToTensor
from torchvision.ops import nms, box_iou
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.models.detection.faster_rcnn import (
    FastRCNNPredictor, FasterRCNN_ResNet50_FPN_Weights
)
from torchmetrics.detection.mean_ap import MeanAveragePrecision

# CONFIG
PROJECT_ROOT = Path("/content/drive/MyDrive/sonar-object-detection")
DATA_ROOT    = PROJECT_ROOT / "data" / "line2voc"
IMAGE_SET    = "val"
CKPT_PATH    = PROJECT_ROOT / "checkpoints" / "dccan_sonar_fasterrcnn_stabilized_25e.pth"

OUT_DIR      = PROJECT_ROOT / "outputs" / "eval_dccan_allinone_25e"
OUT_DIR.mkdir(parents=True, exist_ok=True)

RAW_CSV = OUT_DIR / f"preds_dccan_25epoch_{IMAGE_SET}_RAW.csv"
VIS_CSV = OUT_DIR / f"preds_dccan_25epoch_{IMAGE_SET}_VIS.csv"
VIS_DIR = OUT_DIR / f"vis_{IMAGE_SET}"
VIS_DIR.mkdir(parents=True, exist_ok=True)

CLASS_NAME   = {1: "object", 2: "shadow"}
CLASS_COLORS = {1: (0, 255, 0), 2: (0, 0, 255)}
MAX_DETS_PER_IMAGE = 300

# VIS-only knobs
NMS_IOU_CLASSWISE    = 0.50
MERGE_IOU_CLASSWISE  = 0.60
CROSS_CLASS_IOU      = 0.85
CONF_THRESH_VIS      = 0.50
DO_GREEDY_FUSION     = True
DO_CROSS_CLASS_SUPPR = True

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
torch.backends.cudnn.benchmark = True
print("Device:", device)

# Dataset wrapper
sys.path.append(str(PROJECT_ROOT / "scripts"))
from voc_dataset import VOCDataset

# Build model
def build_model(checkpoint_path: Path):
    weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT
    model = fasterrcnn_resnet50_fpn(weights=weights)
    in_feats = model.roi_heads.box_predictor.cls_score.in_features
    model.roi_heads.box_predictor = FastRCNNPredictor(in_feats, num_classes=3)
    model.roi_heads.score_thresh = 0.0
    model.roi_heads.nms_thresh = 0.5
    model.roi_heads.detections_per_img = MAX_DETS_PER_IMAGE

    state = torch.load(str(checkpoint_path), map_location=device)
    if isinstance(state, dict) and "model" in state:
        state = state["model"]
    model.load_state_dict(state)
    return model.to(device).eval()

# visual dedup helpers
def _classwise_nms(boxes, scores, labels, iou_thr):
    if boxes.numel() == 0:
        return boxes, scores, labels
    keep_all = []
    for c in labels.unique().tolist():
        idx = torch.nonzero(labels == c, as_tuple=False).squeeze(1)
        if idx.numel() == 0:
            continue
        kept = nms(boxes[idx], scores[idx], iou_thr)
        keep_all.append(idx[kept])
    if not keep_all:
        return boxes[:0], scores[:0], labels[:0]
    keep = torch.cat(keep_all, dim=0)
    return boxes[keep], scores[keep], labels[keep]

def _greedy_merge_per_class(boxes, scores, labels, iou_thr):
    if boxes.numel() == 0:
        return boxes, scores, labels
    out_b, out_s, out_l = [], [], []
    for c in labels.unique().tolist():
        idx = torch.nonzero(labels == c, as_tuple=False).squeeze(1)
        if idx.numel() == 0:
            continue
        B, S = boxes[idx].clone(), scores[idx].clone()
        order = torch.argsort(S, descending=True)
        used = torch.zeros(order.numel(), dtype=torch.bool)
        for r, j in enumerate(order):
            if used[r]: continue
            base = B[j].unsqueeze(0)
            ious = box_iou(base, B[order])[0]
            mask = (ious >= iou_thr) & (~used)
            members = order[mask]
            w = S[members]
            fused = (B[members] * w[:, None]).sum(0) / (w.sum() + 1e-8)
            out_b.append(fused); out_s.append(S[members].max()); out_l.append(torch.tensor(c))
            used |= mask
    return torch.stack(out_b), torch.stack(out_s), torch.stack(out_l)

def _cross_class_suppress(boxes, scores, labels, iou_thr):
    if boxes.numel() == 0: return boxes, scores, labels
    order = torch.argsort(scores, descending=True)
    keep = torch.ones(order.numel(), dtype=torch.bool)
    for i in range(order.numel()):
        if not keep[i]: continue
        bi, li = boxes[order[i]].unsqueeze(0), labels[order[i]]
        rest = order[i+1:]
        if rest.numel() == 0: continue
        ious = box_iou(bi, boxes[rest])[0]
        clash = (ious >= iou_thr) & (labels[rest] != li)
        keep[i+1:][clash] = False
    final = order[keep]
    return boxes[final], scores[final], labels[final]

def _enforce_topk(boxes, scores, labels, k):
    if scores.numel() <= k: return boxes, scores, labels
    idx = torch.argsort(scores, descending=True)[:k]
    return boxes[idx], scores[idx], labels[idx]

# eval functions
def coco_eval_from_model(model, dataset_root, image_set, batch_size=4, num_workers=2):
    ds = VOCDataset(str(dataset_root), image_set=image_set, transforms=ToTensor())
    dl = DataLoader(ds, batch_size=batch_size, shuffle=False,
                    collate_fn=lambda x: tuple(zip(*x)), num_workers=num_workers, pin_memory=True)
    metric = MeanAveragePrecision(class_metrics=True)
    metric.warn_on_many_detections = False

    for imgs, targets in dl:
        imgs = [img.to(device) for img in imgs]
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]
        with torch.no_grad(): outs = model(imgs)
        preds, gts = [], []
        for o, t in zip(outs, targets):
            preds.append({"boxes": o["boxes"].cpu(), "scores": o["scores"].cpu(), "labels": o["labels"].cpu()})
            gts.append({"boxes": t["boxes"].cpu(), "labels": t["labels"].cpu()})
        metric.update(preds, gts)

    res = metric.compute()
    print("\n-- Direct COCO (raw model outputs) --")
    for k, v in res.items():
        if isinstance(v, torch.Tensor) and v.ndim == 0:
            print(f"{k}: {float(v):.4f}")
    return res

# main
if __name__ == "__main__":
    model = build_model(CKPT_PATH)
    direct_res = coco_eval_from_model(model, DATA_ROOT, IMAGE_SET, batch_size=8, num_workers=2)

# VISUALIZE PREDICTIONS: DCCAN 25 epochs with raw target split

import sys
import torch
import cv2
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from torchvision.transforms import ToTensor
from torchvision.ops import nms
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.models.detection.faster_rcnn import (
    FastRCNNPredictor,
    FasterRCNN_ResNet50_FPN_Weights,
)

# Config
CONF_THRESH   = 0.50         # draw boxes with score >= 0.50
IOU_THRESH    = 0.65         # IoU used by the optional extra NMS I implemented
NMS_MODE      = "classwise"  # Can be "agnostic" or "classwise", but I chose classwise
NUM_IMAGES    = 5            # number of validation images to visualize

# Paths
PROJECT_ROOT  = Path("/content/drive/MyDrive/sonar-object-detection")
DATASET_ROOT  = str(PROJECT_ROOT / "data" / "line2voc")
CKPT          = str(PROJECT_ROOT / "checkpoints" / "dccan_sonar_fasterrcnn_stabilized_25e.pth")

# Labels & colors, consistent across scripts
LABEL_NAMES   = {1: "object", 2: "shadow"}
LABEL_COLORS  = {1: (0, 255, 0), 2: (0, 0, 255)}  # OpenCV BGR

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Dataset wrapper
sys.path.append(str(PROJECT_ROOT / "scripts"))
from voc_dataset import VOCDataset

# Helper: optional extra NMS for visuals only
def apply_extra_nms(boxes, scores, labels, iou_thresh=0.5, mode="agnostic"):
    if boxes.numel() == 0:
        return torch.empty((0,), dtype=torch.long)

    if mode == "classwise":
        keep_idxs = []
        for cls in torch.unique(labels):
            mask = labels == cls
            if mask.any():
                kept = nms(boxes[mask], scores[mask], iou_thresh)
                keep_idxs.append(torch.where(mask)[0][kept])
        return torch.cat(keep_idxs) if keep_idxs else torch.empty((0,), dtype=torch.long)

    # default: class-agnostic
    return nms(boxes, scores, iou_thresh)

# Model
weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT
model = fasterrcnn_resnet50_fpn(weights=weights)

# Swap head to 3 classes: bg=0, object=1, shadow=2
in_features = model.roi_heads.box_predictor.cls_score.in_features
model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes=3)

# Load DCCAN detector weights
state = torch.load(CKPT, map_location=device)
if isinstance(state, dict) and "model" in state:
    state = state["model"]
model.load_state_dict(state)

# Hygiene for VIS ONLY to prevent hidden pruning inside ROIHeads
model.roi_heads.score_thresh = 0.0          # don't drop boxes internally
model.roi_heads.nms_thresh = 0.5            # to make default explicit
model.roi_heads.detections_per_img = 300    # to avoid silent truncation

model.to(device).eval()

# Visualize a few val images
def visualize(model, dataset_root, num_images, conf_thresh, iou_thresh, nms_mode):
    ds = VOCDataset(dataset_root, image_set="val", transforms=ToTensor())

    for idx in range(min(num_images, len(ds))):
        img, _ = ds[idx]
        x = img.unsqueeze(0).to(device)

        with torch.no_grad():
            out = model(x)[0]

        # dedup just for visuals
        keep = apply_extra_nms(out["boxes"], out["scores"], out["labels"],
                               iou_thresh=iou_thresh, mode=nms_mode)
        boxes  = out["boxes"][keep]
        scores = out["scores"][keep]
        labels = out["labels"][keep]

        # Convert to OpenCV BGR for drawing
        im = cv2.cvtColor(
            (img.mul(255).byte().permute(1, 2, 0).cpu().numpy()),
            cv2.COLOR_RGB2BGR
        )

        shown = 0
        for b, l, s in zip(boxes, labels, scores):
            s = float(s)
            if s < conf_thresh:
                continue
            x1, y1, x2, y2 = b.int().tolist()
            color = LABEL_COLORS.get(int(l), (255, 255, 255))
            name  = LABEL_NAMES.get(int(l), "unknown")

            cv2.rectangle(im, (x1, y1), (x2, y2), color, 2)
            cv2.putText(im, f"{name} {s:.2f}", (x1, max(10, y1 - 6)),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
            shown += 1

        print(
            f"DCCAN (25e) | Image {idx+1}: drew {shown} boxes (score ≥ {conf_thresh:.2f}) "
            f"after {nms_mode} NMS at IoU={iou_thresh:.2f}"
        )

        # Show figure
        plt.figure(figsize=(10, 6))
        plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))
        plt.axis("off")
        plt.title(f"DCCAN (25e) Image {idx+1} | {nms_mode} NMS IoU={iou_thresh}")
        plt.tight_layout(pad=0)
        plt.show()

# Run
visualize(model, DATASET_ROOT, NUM_IMAGES, CONF_THRESH, IOU_THRESH, NMS_MODE)

"""## **Model Comparison from CSVs — COCO (mAP/mAP@50) and FROC**

This cell compares several detectors **from their prediction CSVs** on the `line2voc` **validation** split. It computes **COCO metrics** (including **mAP@50**) using TorchMetrics and plots **FROC** (Recall vs FPPI) — all directly from the CSV files.

> Note: This evaluates *CSV predictions as provided*. If a CSV is already de-duplicated or score-filtered, the resulting metrics will reflect that post-processing.
"""

# Fair evaluation of my models: TorchMetrics COCO mAP + FROC for each model
from pathlib import Path
import xml.etree.ElementTree as ET
import warnings

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import torch
from torchvision.ops import box_iou
from torchmetrics.detection.mean_ap import MeanAveragePrecision


# Config
PROJECT_ROOT = Path("/content/drive/MyDrive/sonar-object-detection")
OUT_DIR      = PROJECT_ROOT / "outputs" / "models_comparison"
OUT_DIR.mkdir(parents=True, exist_ok=True)

EVAL_MAX_DETS = 100

# Each row: Display Name, CSV path, GT root, split txt
CSV_SPECS = [
    ("Raw (20ep)",
     PROJECT_ROOT / "outputs" / "preds_baseline_frcnn_20epoch.csv",
     PROJECT_ROOT / "data" / "line2voc",
     "val.txt"),

    ("Denoised (20ep)",
     PROJECT_ROOT / "outputs" / "preds_denoised_frcnn_20epoch.csv",
     PROJECT_ROOT / "data" / "line2voc",
     "val.txt"),

    # IMPORTANT: CLAHE+Aug evaluated on the augmented dataset root
    ("CLAHE+Aug (20ep)",
     PROJECT_ROOT / "outputs" / "preds_claheaug_frcnn_20epoch.csv",
     PROJECT_ROOT / "data" / "line2voc_preprocessed_augmented",
     "val.txt"),

    ("DANN (20ep)",
     PROJECT_ROOT / "outputs" / "eval_dann_allinone" / "preds_dann_20epoch_val_RAW.csv",
     PROJECT_ROOT / "data" / "line2voc",
     "val.txt"),

    # DCCAN: use RAW CSV exported by the batch inference script
    ("DCCAN (20ep)",
     PROJECT_ROOT / "outputs" / "preds_dccan_20epoch_RAW.csv",
     PROJECT_ROOT / "data" / "line2voc",
     "val.txt"),
]

# Eval knobs
CLASS_NAME       = {1: "object", 2: "shadow"}   # for reporting
REQUIRE_COLUMNS  = {"filename","class_id","score","xmin","ymin","xmax","ymax"}

# FROC settings
FROC_IOU_MATCH   = 0.50
FROC_THRESHOLDS  = np.linspace(0.05, 0.95, 10)


# VOC GT utilities
def load_voc_gt_for_root(voc_root: Path, split_txt: str, class_map={"object":1,"shadow":2}):
    """Load GT boxes/labels + image sizes for a specific dataset root/split."""
    voc_root = Path(voc_root)
    split_path = voc_root / "ImageSets" / "Main" / split_txt
    ids = split_path.read_text().splitlines()
    out = {}
    sizes = {}
    for img_id in ids:
        xml_path = voc_root / "Annotations" / f"{img_id}.xml"
        boxes, labels = [], []
        try:
            root = ET.parse(xml_path).getroot()
            w = int(root.find("size/width").text); h = int(root.find("size/height").text)
            sizes[f"{img_id}.jpg"] = (w, h)
            for obj in root.findall("object"):
                name = obj.find("name").text.strip()
                cid = class_map.get(name)
                if cid is None:
                    continue
                bb = obj.find("bndbox")
                xmin=float(bb.find("xmin").text); ymin=float(bb.find("ymin").text)
                xmax=float(bb.find("xmax").text); ymax=float(bb.find("ymax").text)
                if xmax > xmin and ymax > ymin:
                    boxes.append([xmin,ymin,xmax,ymax]); labels.append(cid)
        except Exception as e:
            warnings.warn(f"Failed to parse {xml_path}: {e}")
        out[f"{img_id}.jpg"] = (
            torch.tensor(boxes, dtype=torch.float32),
            torch.tensor(labels, dtype=torch.int64),
        )
    return out, set(f"{x}.jpg" for x in ids), sizes


# CSV sanitization
def sanitize_csv(df: pd.DataFrame, filenames_keep: set, img_sizes: dict) -> pd.DataFrame:
    df = df.copy()

    # filenames -> basename only and force .jpg to match VOC list
    df["filename"] = df["filename"].astype(str).apply(lambda x: Path(x).name)
    df["filename"] = df["filename"].apply(lambda s: Path(s).stem + ".jpg")

    # keep only rows in split
    df = df[df["filename"].isin(filenames_keep)]

    # class ids: fix {0,1} -> {1,2}
    uniq = set(df["class_id"].unique())
    if uniq == {0,1}:
        df["class_id"] = df["class_id"] + 1

    # coords: if normalized (<=1), rescale using image size from GT XML
    if not df.empty and df[["xmin","ymin","xmax","ymax"]].max().max() <= 1.001:
        def _rescale(row):
            w,h = img_sizes.get(row["filename"], (None,None))
            if w is not None:
                row["xmin"] *= w; row["xmax"] *= w
                row["ymin"] *= h; row["ymax"] *= h
            return row
        df = df.apply(_rescale, axis=1)

    # drop invalid boxes
    df = df[(df["xmax"] > df["xmin"]) & (df["ymax"] > df["ymin"])]

    return df


# Build preds,GTs for TorchMetrics from a CSV and specific GT
def load_predictions_from_csv_for_gt(csv_path: Path,
                                     gt_map: dict,
                                     filenames_keep: set,
                                     img_sizes: dict,
                                     score_min: float | None = None,
                                     max_dets_per_img: int | None = EVAL_MAX_DETS):
    df = pd.read_csv(csv_path)
    if not REQUIRE_COLUMNS.issubset(df.columns):
        missing = REQUIRE_COLUMNS - set(df.columns)
        raise RuntimeError(f"{csv_path} missing columns: {missing}")

    df = sanitize_csv(df, filenames_keep, img_sizes)

    # score gate
    if score_min is not None:
        df = df[df["score"] >= float(score_min)]

    # filename -> tensors
    preds_by_img = {}
    for fname, g in df.groupby("filename", sort=False):
        g = g.sort_values("score", ascending=False)
        if max_dets_per_img is not None:
            g = g.head(int(max_dets_per_img))
        preds_by_img[fname] = {
            "boxes":  torch.tensor(g[["xmin","ymin","xmax","ymax"]].values, dtype=torch.float32),
            "scores": torch.tensor(g["score"].values, dtype=torch.float32),
            "labels": torch.tensor(g["class_id"].values, dtype=torch.int64),
        }

    preds, gts = [], []
    for fname in sorted(filenames_keep):
        pred = preds_by_img.get(fname, {
            "boxes": torch.empty((0,4), dtype=torch.float32),
            "scores": torch.empty((0,),  dtype=torch.float32),
            "labels": torch.empty((0,),  dtype=torch.int64),
        })
        gb, gl = gt_map[fname]
        preds.append(pred)
        gts.append({"boxes": gb, "labels": gl})
    return preds, gts


# TorchMetrics COCO
def _clip_preds(preds: list[dict], cap: int) -> list[dict]:
    out = []
    for d in preds:
        if d["scores"].numel() > cap:
            idx = torch.argsort(d["scores"], descending=True)[:cap]
            out.append({"boxes": d["boxes"][idx], "scores": d["scores"][idx], "labels": d["labels"][idx]})
        else:
            out.append(d)
    return out

def coco_eval_from_csv_for_gt(csv_path: Path,
                              gt_map: dict,
                              filenames_keep: set,
                              img_sizes: dict,
                              score_min: float | None = None,
                              max_dets_per_img: int | None = EVAL_MAX_DETS,
                              class_metrics: bool = True):
    preds, gts = load_predictions_from_csv_for_gt(
        csv_path, gt_map, filenames_keep, img_sizes, score_min, max_dets_per_img
    )
    metric = MeanAveragePrecision(
        class_metrics=class_metrics,
        max_detection_thresholds=[1, 10, EVAL_MAX_DETS],  # COCO-style
        box_format="xyxy",
    )
    metric.warn_on_many_detections = False  # quiet the warning
    metric.update(_clip_preds(preds, EVAL_MAX_DETS), gts)
    return metric.compute()


# FROC
def froc_from_csv_for_gt(csv_path: Path,
                         gt_map: dict,
                         filenames_keep: set,
                         img_sizes: dict,
                         thresholds=FROC_THRESHOLDS,
                         iou_thr=FROC_IOU_MATCH):
    df = pd.read_csv(csv_path)
    if not REQUIRE_COLUMNS.issubset(df.columns):
        missing = REQUIRE_COLUMNS - set(df.columns)
        raise RuntimeError(f"{csv_path} missing columns: {missing}")
    df = sanitize_csv(df, filenames_keep, img_sizes)

    X, Y = [], []
    num_images = len(filenames_keep)

    for t in thresholds:
        TP = 0; FN = 0; FP = 0
        for fname in filenames_keep:
            g = df[(df.filename == fname) & (df.score >= t)].sort_values("score", ascending=False)
            pb = torch.tensor(g[["xmin","ymin","xmax","ymax"]].values, dtype=torch.float32)
            pl = torch.tensor(g["class_id"].values, dtype=torch.int64)

            gb, gl = gt_map[fname]
            if pb.numel() == 0 and gb.numel() == 0:
                continue

            matched_gt = torch.zeros(gl.size(0), dtype=torch.bool)
            matched_pr = torch.zeros(pl.size(0), dtype=torch.bool)

            if pb.numel() > 0 and gb.numel() > 0:
                ious = box_iou(pb, gb)  # [P,G]
                for pi in range(pb.size(0)):
                    same_cls = (pl[pi] == gl)
                    if same_cls.any():
                        cand = ious[pi].clone()
                        cand[~same_cls] = -1.0
                        gi = torch.argmax(cand)
                        if cand[gi] >= iou_thr and not matched_gt[gi]:
                            matched_pr[pi] = True
                            matched_gt[gi] = True

            TP += int(matched_gt.sum().item())
            FN += int((~matched_gt).sum().item())
            FP += int((~matched_pr).sum().item())

        recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0
        fppi   = FP / max(1, num_images)
        X.append(fppi); Y.append(recall)

    return np.array(X), np.array(Y)


# Main
def main():
    rows_main = []
    per_class_rows = []

    fig, axes = plt.subplots(1, 2, figsize=(13, 5))
    ax_bar, ax_froc = axes

    for name, csv_path, gt_root, split_txt in CSV_SPECS:
        if not csv_path.exists():
            warnings.warn(f"Missing CSV for {name}: {csv_path}")
            continue

        # Load correct GT for this model
        GT_map, FILENAMES, IMG_SIZES = load_voc_gt_for_root(gt_root, split_txt, class_map={"object":1,"shadow":2})

        # ---- COCO from CSV ----
        res = coco_eval_from_csv_for_gt(csv_path, GT_map, FILENAMES, IMG_SIZES,
                                        score_min=None, max_dets_per_img=EVAL_MAX_DETS, class_metrics=True)

        # Summary row
        rows_main.append({
            "Model": name,
            "mAP":    float(res["map"].item()),
            "mAP@50": float(res["map_50"].item()),
            "mAP@75": float(res["map_75"].item()),
            "mAR@100":float(res["mar_100"].item()),
        })

        # Per-class
        cls, map_pc, mar_pc = res.get("classes"), res.get("map_per_class"), res.get("mar_100_per_class")
        if isinstance(cls, torch.Tensor) and isinstance(map_pc, torch.Tensor):
            mar_list = mar_pc.detach().cpu().tolist() if isinstance(mar_pc, torch.Tensor) else [None]*len(cls)
            for cid, m, r in zip(cls.cpu().tolist(),
                                 map_pc.detach().cpu().tolist(),
                                 mar_list):
                per_class_rows.append({
                    "Model": name,
                    "ClassID": cid,
                    "ClassName": CLASS_NAME.get(int(cid), str(cid)),
                    "mAP_per_class": float(m),
                    "mAR@100_per_class": float(r) if r is not None else None,
                })

        # FROC ALL predictions
        try:
            x, y = froc_from_csv_for_gt(csv_path, GT_map, FILENAMES, IMG_SIZES,
                                        thresholds=FROC_THRESHOLDS, iou_thr=FROC_IOU_MATCH)
            ax_froc.plot(x, y, marker="o", ms=3, label=name)
        except Exception as e:
            warnings.warn(f"FROC failed for {name}: {e}")

    # Save tables
    df_main = pd.DataFrame(rows_main).sort_values("mAP@50", ascending=False).reset_index(drop=True)
    df_main.to_csv(OUT_DIR / "eval_coco_from_csv_summary.csv", index=False)

    if per_class_rows:
        df_pc = pd.DataFrame(per_class_rows)
        df_pc.to_csv(OUT_DIR / "eval_coco_from_csv_per_class.csv", index=False)

    # Left: mAP@50 bar
    ax_bar.bar(df_main["Model"], df_main["mAP@50"])
    ax_bar.set_ylabel("mAP@50 (COCO, from CSV)")
    ax_bar.set_title("Detection Quality (CSV -> TorchMetrics COCO)")
    ax_bar.tick_params(axis="x", rotation=15)

    # Right: FROC
    ax_froc.set_xlabel("False Positives per Image (FPPI)")
    ax_froc.set_ylabel(f"Recall (IoU ≥ {FROC_IOU_MATCH:.2f})")
    ax_froc.set_title("FROC (CSV predictions)")
    ax_froc.grid(True, alpha=0.3)
    ax_froc.legend()

    fig.tight_layout()
    fig.savefig(OUT_DIR / "comparison_panel_coco_from_csv_and_froc.png", dpi=200)
    plt.show()

    print("\n== COCO mAP from CSVs using TorchMetrics ==")
    print(df_main.round(4))
    print(f"\nSaved CSV  -> {OUT_DIR / 'eval_coco_from_csv_summary.csv'}")
    if per_class_rows:
        print(f"Per-class  -> {OUT_DIR / 'eval_coco_from_csv_per_class.csv'}")
    print(f"Figure     -> {OUT_DIR / 'comparison_panel_coco_from_csv_and_froc.png'}")

if __name__ == "__main__":
    main()