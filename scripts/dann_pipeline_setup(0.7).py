# -*- coding: utf-8 -*-
"""DANN_Pipeline_Setup(0.7).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E56NygZUvuj4dY73jCvMq4rzXr-_lfA7

# **MSc Thesis Phase 2: Domain Adaptation with DANN**

This notebook implements domain-adversarial training (DANN) for object detection in sidescan sonar images. The model is trained using a source domain (labeled) and target domain (unlabeled) to improve generalization under domain shift.

## **Environment Setup**
"""

# Commented out IPython magic to ensure Python compatibility.
# securely access project private repository
import os
os.environ["GITHUB_TOKEN"] = "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"
!git clone https://${GITHUB_TOKEN}@github.com/Kablan-ASBN/sonar-object-detection.git
# %cd sonar-object-detection

# Mount Google Drive to access dataset
from google.colab import drive
drive.mount('/content/drive')

# Dependencies
!pip install -r requirements.txt
!pip uninstall -y tllib
!pip install git+https://github.com/thuml/Transfer-Learning-Library.git

# Patch for Python 3.11 compatibility
!sed -i "s/from torch.utils.data.dataset import Dataset, T_co, IterableDataset/from torch.utils.data import Dataset, IterableDataset/" /usr/local/lib/python3.11/dist-packages/tllib/vision/datasets/imagelist.py
!sed -i "s/class MultipleDomainsDataset(Dataset\\[T_co\\]):/class MultipleDomainsDataset(Dataset):/" /usr/local/lib/python3.11/dist-packages/tllib/vision/datasets/imagelist.py
!sed -i "s/List\\[Dataset\\[T_co\\]\\]/List[Dataset]/g" /usr/local/lib/python3.11/dist-packages/tllib/vision/datasets/imagelist.py
!sed -i "s/T_co/int/g" /usr/local/lib/python3.11/dist-packages/tllib/vision/datasets/imagelist.py

"""## **Rebuild train.txt from JPEGImages**"""

from pathlib import Path

def rebuild_train_txt(folder):
    folder = Path(folder)
    jpeg_dir = folder / "JPEGImages"
    image_ids = [f.stem for f in jpeg_dir.glob("*.jpg")]
    image_ids.sort()

    txt_path = folder / "ImageSets/Main/train.txt"
    txt_path.parent.mkdir(parents=True, exist_ok=True)

    with open(txt_path, "w") as f:
        f.write("\n".join(image_ids))

    print(f"{folder.name} train.txt rebuilt: {len(image_ids)} entries")

rebuild_train_txt("data/line2voc_preprocessed")
rebuild_train_txt("data/line2voc")

"""## **Sanitize Filenames**"""

from pathlib import Path

def sanitize_file_names(folder):
    folder = Path(folder)
    jpeg_dir = folder / "JPEGImages"
    ann_dir = folder / "Annotations"
    txt_file = folder / "ImageSets/Main/train.txt"

    # Rename JPEG files
    for file in jpeg_dir.glob("*.jpg"):
        new_name = file.name.replace(" ", "_")
        if file.name != new_name:
            file.rename(jpeg_dir / new_name)

    # Rename XML annotation files
    for file in ann_dir.glob("*.xml"):
        new_name = file.name.replace(" ", "_")
        if file.name != new_name:
            file.rename(ann_dir / new_name)

    # Update train.txt
    if txt_file.exists():
        with open(txt_file, "r") as f:
            ids = [line.strip().replace(" ", "_") for line in f.readlines()]
        with open(txt_file, "w") as f:
            f.write("\n".join(ids))

    print(f"{folder.name} sanitized.")

sanitize_file_names("data/line2voc_preprocessed")
sanitize_file_names("data/line2voc")

"""## **Clean & Deduplicate train.txt**"""

from pathlib import Path

def clean_and_deduplicate(folder):
    folder = Path(folder)
    img_dir = folder / "JPEGImages"
    ann_dir = folder / "Annotations"
    txt_path = folder / "ImageSets/Main/train.txt"

    # Read and deduplicate IDs
    with open(txt_path, "r") as f:
        ids = set(line.strip() for line in f)

    # Only keep IDs with both .jpg and .xml files
    valid_ids = [img_id for img_id in sorted(ids)
                 if (img_dir / f"{img_id}.jpg").exists() and (ann_dir / f"{img_id}.xml").exists()]

    # Save back cleaned list
    with open(txt_path, "w") as f:
        f.write("\n".join(valid_ids))

    print(f"{folder.name}: {len(valid_ids)} valid entries")

clean_and_deduplicate("data/line2voc_preprocessed")
clean_and_deduplicate("data/line2voc")

"""## **Final Consistency Check**"""

from pathlib import Path

def verify_dataset(folder):
    folder = Path(folder)
    img_dir = folder / "JPEGImages"
    ann_dir = folder / "Annotations"
    txt_path = folder / "ImageSets/Main/train.txt"

    with open(txt_path, "r") as f:
        ids = [line.strip() for line in f]

    missing_imgs = [img_id for img_id in ids if not (img_dir / f"{img_id}.jpg").exists()]
    missing_anns = [img_id for img_id in ids if not (ann_dir / f"{img_id}.xml").exists()]

    print(f"\n {folder.name}")
    print(f"  Total entries in train.txt: {len(ids)}")
    print(f"  Missing JPEGs: {len(missing_imgs)}")
    print(f"  Missing XMLs: {len(missing_anns)}")

verify_dataset("data/line2voc_preprocessed")
verify_dataset("data/line2voc")

"""## **Dataset & DataLoader**"""

import os
import xml.etree.ElementTree as ET
from PIL import Image
import torch
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as T

class CustomVOCDataset(Dataset):
    def __init__(self, root, image_set="train", transforms=None):
        self.root = root
        self.image_dir = os.path.join(root, "JPEGImages")
        self.annotation_dir = os.path.join(root, "Annotations")
        self.ids = open(os.path.join(root, "ImageSets/Main", f"{image_set}.txt")).read().strip().split()
        self.transforms = transforms if transforms else T.ToTensor()

    def __len__(self):
        return len(self.ids)

    def __getitem__(self, idx):
        img_id = self.ids[idx]
        img_path = os.path.join(self.image_dir, f"{img_id}.jpg")
        ann_path = os.path.join(self.annotation_dir, f"{img_id}.xml")

        img = Image.open(img_path).convert("RGB")

        boxes, labels = [], []
        if os.path.exists(ann_path):
            try:
                tree = ET.parse(ann_path)
                root = tree.getroot()
                for obj in root.findall("object"):
                    bbox = obj.find("bndbox")
                    xmin = float(bbox.find("xmin").text)
                    ymin = float(bbox.find("ymin").text)
                    xmax = float(bbox.find("xmax").text)
                    ymax = float(bbox.find("ymax").text)

                    # Only keep boxes with valid area
                    if (
                        xmin < xmax and ymin < ymax and
                        all(torch.isfinite(torch.tensor([xmin, ymin, xmax, ymax])))
                    ):
                        boxes.append([xmin, ymin, xmax, ymax])
                        labels.append(1)
            except Exception as e:
                print(f"Error parsing {ann_path}: {e}")

        # If no valid boxes found, it inserts a dummy box
        if len(boxes) == 0:
            boxes = [[0.0, 0.0, 1.0, 1.0]]
            labels = [0]  # treat as background

        target = {
            "boxes": torch.tensor(boxes, dtype=torch.float32),
            "labels": torch.tensor(labels, dtype=torch.int64),
            "image_id": torch.tensor([idx])
        }

        if self.transforms:
            img = self.transforms(img)

        return img, target

"""## **DataLoader Setup**"""

# Collate function for Faster R-CNN model
def collate_fn(batch):
    return list(zip(*batch))

# Transform
transform = T.Compose([
    T.ToTensor()
])

# Create datasets
source_dataset = CustomVOCDataset("data/line2voc_preprocessed", transforms=transform)
target_dataset = CustomVOCDataset("data/line2voc", transforms=transform)

# Create dataloaders
source_loader = DataLoader(source_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)
target_loader = DataLoader(target_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)

# Confirm size
print(f"Source dataset size: {len(source_dataset)}")
print(f"Target dataset size: {len(target_dataset)}")

"""## **Model Setup**"""

import torch
import torchvision
from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights
from torch import nn
import numpy as np

# Fix deprecated numpy alias used by torchvision (Was getting a bug without it)
np.float = float

# Device configuration ("cuda" will be use if GPU available, else the "cpu" will be used)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# Load COCO-pretrained Faster R-CNN
weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT
model = fasterrcnn_resnet50_fpn(weights=weights)

# Replace classifier with 2-class head (background & sonar object)
in_features = model.roi_heads.box_predictor.cls_score.in_features
model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(
    in_features, num_classes=2
)

model.to(device)

# Setup DANN modules
from tllib.alignment.dann import DomainAdversarialLoss
from tllib.modules.domain_discriminator import DomainDiscriminator

domain_discriminator = DomainDiscriminator(in_feature=256, hidden_size=1024)
domain_loss_fn = DomainAdversarialLoss(domain_discriminator=domain_discriminator).to(device)

"""## **DANN Training Loop (5 Epochs)**"""

import os
from tqdm import tqdm
import torch
import torch.nn.functional as F

# Debug mode: ensures CUDA errors appear on the right line
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'

# Optimizer (allows stable learning rate)
optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0005)

model.train()
domain_discriminator.train()

epochs = 5
for epoch in range(epochs):
    total_det_loss = 0.0
    total_domain_loss = 0.0

    source_iter = iter(source_loader)
    target_iter = iter(target_loader)
    min_batches = min(len(source_loader), len(target_loader))

    loop = tqdm(range(min_batches), desc=f"Epoch {epoch+1}/{epochs}")

    for _ in loop:
        try:
            source_images, source_targets = next(source_iter)
            target_images, _ = next(target_iter)
        except StopIteration:
            break

        source_images = [img.to(device) for img in source_images]
        target_images = [img.to(device) for img in target_images]
        source_targets = [{k: v.to(device) for k, v in t.items()} for t in source_targets]

        # ==== Detection Loss ====
        try:
            loss_dict = model(source_images, source_targets)
            det_loss = sum(loss_dict.values())
        except Exception as e:
            print(f"Detection loss error, skipping batch: {e}")
            continue

        # Optional logging
        if not torch.isfinite(det_loss):
            print("Non-finite detection loss detected, skipping batch.")
            continue

        if det_loss.item() > 1000:
            print(f"Warning: High detection loss {det_loss.item()}")

        # ==== Domain Adaptation Loss ====
        with torch.no_grad():
            f_s_dict = model.backbone(torch.stack(source_images))
            f_t_dict = model.backbone(torch.stack(target_images))

        f_s = F.adaptive_avg_pool2d(f_s_dict["0"], (1, 1)).squeeze(-1).squeeze(-1)
        f_t = F.adaptive_avg_pool2d(f_t_dict["0"], (1, 1)).squeeze(-1).squeeze(-1)

        try:
            domain_loss = domain_loss_fn(f_s, f_t)
        except Exception as e:
            print(f"Domain loss error, skipping batch: {e}")
            continue

        if not torch.isfinite(domain_loss):
            print("Non-finite domain loss detected, skipping batch.")
            continue

        if domain_loss.item() > 1000:
            print(f"Warning: High domain loss {domain_loss.item()}")

        # ==== Total Loss ====
        total_loss = det_loss + domain_loss

        optimizer.zero_grad()
        total_loss.backward()
        optimizer.step()

        total_det_loss += det_loss.item()
        total_domain_loss += domain_loss.item()

        loop.set_postfix({
            "DetLoss": f"{det_loss.item():.4f}",
            "DomLoss": f"{domain_loss.item():.4f}"
        })

    print(f"Epoch {epoch+1}/{epochs} | Detection Loss: {total_det_loss:.4f} | Domain Loss: {total_domain_loss:.4f}")

"""## **Saving the DANN-trained model**"""

torch.save(model.state_dict(), "/content/drive/MyDrive/sonar-object-detection/checkpoints/dann_sonar_fasterrcnn.pth")

"""## **Batch Inference on Target Domain**"""

from pathlib import Path
import torch
import torchvision.transforms as T
from PIL import Image
import pandas as pd
from torchvision.ops import nms
from tqdm import tqdm

# Define transform (same as training)
transform = T.Compose([
    T.ToTensor()
])

# Inference function
def run_dann_inference(model, image_dir, output_csv, score_thresh=0.7, iou_thresh=0.3):
    model.eval()
    results = []

    image_dir = Path(image_dir)
    image_paths = sorted(image_dir.glob("*.jpg"))

    for image_path in tqdm(image_paths, desc="Running DANN Inference"):
        image = Image.open(image_path).convert("RGB")
        tensor = transform(image).unsqueeze(0).to(device)

        with torch.no_grad():
            outputs = model(tensor)

        boxes = outputs[0]['boxes'].cpu()
        scores = outputs[0]['scores'].cpu()
        labels = outputs[0]['labels'].cpu()

        keep = nms(boxes, scores, iou_thresh)
        boxes = boxes[keep]
        scores = scores[keep]
        labels = labels[keep]

        for box, score, label in zip(boxes, scores, labels):
            if score >= score_thresh:
                xmin, ymin, xmax, ymax = box.tolist()
                results.append({
                    "filename": image_path.name,
                    "class_id": int(label),
                    "score": float(score),
                    "xmin": xmin,
                    "ymin": ymin,
                    "xmax": xmax,
                    "ymax": ymax
                })

    df = pd.DataFrame(results)
    df.to_csv(output_csv, index=False)
    return df

# Usage
target_image_dir = "data/line2voc/JPEGImages"
output_csv_path = "/content/drive/MyDrive/sonar-object-detection/outputs/preds_dann.csv"

# Run on trained model
df_preds = run_dann_inference(model, target_image_dir, output_csv_path)

"""## **DANN Predictions Visualization**"""

import os
import cv2
import pandas as pd
from pathlib import Path
from tqdm import tqdm

def visualize_predictions(csv_path, image_dir, output_dir, score_thresh=0.5):
    df = pd.read_csv(csv_path)
    image_dir = Path(image_dir)
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    grouped = df[df["score"] >= score_thresh].groupby("filename")

    for filename, group in tqdm(grouped, desc="Visualizing Predictions"):
        img_path = image_dir / filename
        img = cv2.imread(str(img_path))
        if img is None:
            continue

        for _, row in group.iterrows():
            xmin, ymin, xmax, ymax = map(int, [row.xmin, row.ymin, row.xmax, row.ymax])
            cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)
            label = f"{row.class_id}:{row.score:.2f}"
            cv2.putText(img, label, (xmin, max(0, ymin - 5)),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)

        out_path = output_dir / filename
        cv2.imwrite(str(out_path), img)

# Run prediction
visualize_predictions(
    csv_path="/content/drive/MyDrive/sonar-object-detection/outputs/preds_dann.csv",
    image_dir="data/line2voc/JPEGImages",
    output_dir="/content/drive/MyDrive/sonar-object-detection/outputs/dann_vis"
)

"""## **Quantitative Comparison of DANN vs Baseline Model**
*Visualization for the report and make it easier to understand results*
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.pyplot as plt
import seaborn as sns

# Phase 1: Load Predictions


# Load prediction results from Phase 1 (baseline) and Phase 2 (DANN)
baseline_path = "/content/drive/MyDrive/sonar-object-detection/outputs/preds.csv"
dann_path = "/content/drive/MyDrive/sonar-object-detection/outputs/preds_dann.csv"

# Read CSVs into DataFrames
baseline_preds = pd.read_csv(baseline_path)
dann_preds = pd.read_csv(dann_path)

# Print sanity checks
print(f"Loaded {len(baseline_preds)} baseline predictions")
print(f"Loaded {len(dann_preds)} DANN predictions")

# Phase 2: Per-Image Statistics


# Count number of detections per image for each model
baseline_per_image = baseline_preds.groupby("filename").size()
dann_per_image = dann_preds.groupby("filename").size()

# Build summary statistics
summary_df = pd.DataFrame({
    "Metric": ["Total Detections", "Mean Detections per Image", "Median Detections per Image"],
    "Baseline": [
        len(baseline_preds),
        round(baseline_per_image.mean(), 2),
        baseline_per_image.median()
    ],
    "DANN": [
        len(dann_preds),
        round(dann_per_image.mean(), 2),
        dann_per_image.median()
    ]
})

# Display results as table
from IPython.display import display
display(summary_df)

# Phase 3: Visualization (Barplot)


# Prepare data in long format for seaborn
viz_df = pd.DataFrame({
    "Model": ["Baseline"] * len(baseline_per_image) + ["DANN"] * len(dann_per_image),
    "Detections per Image": pd.concat([baseline_per_image, dann_per_image])
})

# Set visual style
sns.set(style="whitegrid")
plt.figure(figsize=(10, 6))

# Boxplot & stripplot to show distribution
sns.boxplot(data=viz_df, x="Model", y="Detections per Image", palette="pastel")
sns.stripplot(data=viz_df, x="Model", y="Detections per Image", color="gray", alpha=0.2, jitter=True)

plt.title("Comparison of Detections per Image: Baseline vs DANN")
plt.ylabel("Number of Detections")
plt.xlabel("Model")
plt.tight_layout()
plt.show()

# Histogram
plt.figure(figsize=(10, 5))
sns.histplot(baseline_per_image, label="Baseline", color="blue", bins=30, kde=True, stat="count", alpha=0.5)
sns.histplot(dann_per_image, label="DANN", color="green", bins=30, kde=True, stat="count", alpha=0.5)
plt.legend()
plt.title("Distribution of Detections per Image (Baseline vs DANN)")
plt.xlabel("Detections per Image")
plt.ylabel("Number of Images")
plt.tight_layout()
plt.grid(True)
plt.show()

"""## **Sanitize test.txt**
*This step ensures that the test.txt file — which lists the test image IDs — uses consistent filenames. Since the actual image and annotation files had underscores (e.g., Line_2_CNav_...) but test.txt used spaces (e.g., Line 2_CNav_...), this mismatch would lead to file-not-found errors during evaluation. We sanitize test.txt by replacing spaces with underscores in all entries so that it correctly matches the actual filenames in the dataset folders.*
"""

from pathlib import Path

def sanitize_test_txt(voc_root):
    test_txt_path = Path(voc_root) / "ImageSets/Main/test.txt"

    with open(test_txt_path, "r") as f:
        ids = [line.strip().replace(" ", "_") for line in f.readlines()]

    with open(test_txt_path, "w") as f:
        f.write("\n".join(ids))

    print(f" test.txt sanitized with underscores: {len(ids)} entries")

sanitize_test_txt("/content/drive/MyDrive/sonar-object-detection/data/line2voc")

"""## **Rename image files to match test.txt**
*This step renames all .jpg image files and .xml annotation files inside the dataset so they consistently use underscores instead of spaces. Filenames with spaces can cause issues in file handling, especially when building datasets or running batch inference. Renaming these files ensures compatibility with Python's file reading functions and guarantees that all files listed in test.txt actually exist in the JPEGImages/ and Annotations/ folders.*
"""

from pathlib import Path

def rename_files_to_underscores(folder_path):
    folder = Path(folder_path)
    img_dir = folder / "JPEGImages"
    ann_dir = folder / "Annotations"

    renamed = 0
    for img_file in img_dir.glob("*.jpg"):
        new_name = img_file.name.replace(" ", "_")
        if new_name != img_file.name:
            img_file.rename(img_dir / new_name)
            renamed += 1

    for xml_file in ann_dir.glob("*.xml"):
        new_name = xml_file.name.replace(" ", "_")
        if new_name != xml_file.name:
            xml_file.rename(ann_dir / new_name)
            renamed += 1

    print(f"Renamed {renamed} files with underscores.")

rename_files_to_underscores("/content/drive/MyDrive/sonar-object-detection/data/line2voc")

"""## **DANN Model Evaluation**"""

# Install torchmetrics
!pip install -q torchmetrics

# Import required libraries to evaluate the DANN Model
import torch
from torchmetrics.detection.mean_ap import MeanAveragePrecision
from torch.utils.data import DataLoader
from torchvision.transforms import ToTensor
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor

# Load the trained DANN model from checkpoints
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = fasterrcnn_resnet50_fpn(weights=None)
in_features = model.roi_heads.box_predictor.cls_score.in_features
model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes=2)  # background + object

checkpoint_path = "/content/drive/MyDrive/sonar-object-detection/checkpoints/dann_sonar_fasterrcnn.pth"
model.load_state_dict(torch.load(checkpoint_path, map_location=device))
model.to(device)
model.eval()

# Load the test set from the target domain
# CustomVOCDataset was defined earlier in this notebook
test_dataset = CustomVOCDataset(
    root="/content/drive/MyDrive/sonar-object-detection/data/line2voc",
    image_set="test",
    transforms=ToTensor()
)

test_loader = DataLoader(
    test_dataset,
    batch_size=4,
    shuffle=False,
    collate_fn=lambda x: tuple(zip(*x))
)

# Evaluate the model using torchmetrics (Mean Average Precision - mAP)
metric = MeanAveragePrecision()

with torch.no_grad():
    for imgs, targets in test_loader:
        imgs = [img.to(device) for img in imgs]
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

        outputs = model(imgs)

        preds = []
        targs = []

        for o, t in zip(outputs, targets):
            preds.append({
                "boxes": o["boxes"].cpu(),
                "scores": o["scores"].cpu(),
                "labels": o["labels"].cpu()
            })
            targs.append({
                "boxes": t["boxes"].cpu(),
                "labels": t["labels"].cpu()
            })

        metric.update(preds, targs)

# Display evaluation metrics
results = metric.compute()

print("\nDANN Evaluation Results on Target Domain:")
for k, v in results.items():
    print(f"{k}: {v:.4f}")

# Precision/Recall using true counts of TP, FP, FN
tp = 0  # True positives: predicted box matched a ground truth box
fp = 0  # False positives: predicted box matched nothing
fn = 0  # False negatives: ground truth box was missed

iou_threshold = 0.5

def compute_iou(boxA, boxB):
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[2], boxB[2])
    yB = min(boxA[3], boxB[3])
    interArea = max(0, xB - xA) * max(0, yB - yA)
    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])
    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])
    return interArea / float(boxAArea + boxBArea - interArea + 1e-6)

# Set model to evaluation mode (inference only)
model.eval()
with torch.no_grad():
    for imgs, targets in test_loader:
        imgs = [img.to(device) for img in imgs]
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]
        outputs = model(imgs)

        for output, target in zip(outputs, targets):
            pred_boxes = output['boxes'].cpu().numpy()
            true_boxes = target['boxes'].cpu().numpy()

            matched_gt = set()

            # Evaluate each prediction
            for pred_box in pred_boxes:
                match_found = False
                for j, true_box in enumerate(true_boxes):
                    if j in matched_gt:
                        continue
                    iou = compute_iou(pred_box, true_box)
                    if iou >= iou_threshold:
                        tp += 1
                        matched_gt.add(j)
                        match_found = True
                        break
                if not match_found:
                    fp += 1  # this predicted box matched nothing

            # Any ground truth not matched is a false negative
            fn += len(true_boxes) - len(matched_gt)

# Calculate precision and recall using TP, FP, FN
precision = tp / (tp + fp + 1e-6)
recall = tp / (tp + fn + 1e-6)

print(f"True Positives: {tp}")
print(f"False Positives: {fp}")
print(f"False Negatives: {fn}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")